{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_relation(row, entry_syntax):\n",
    "\n",
    "    Lprefix, Rprefix, r = row\n",
    "\n",
    "    rl = ['r','l']\n",
    "    \n",
    "    if (r[0][0].casefold() not in rl) or (r[1][0].casefold() not in rl):\n",
    "        raise ValueError('r l error')\n",
    "        return False\n",
    "    \n",
    "    series_labels = {'r':entry_syntax.loc[entry_syntax['entry_prefix'] == Rprefix],\n",
    "                     'l':entry_syntax.loc[entry_syntax['entry_prefix'] == Lprefix]}\n",
    "\n",
    "    for locator in r[:2]:\n",
    "        if len(locator) > 1:\n",
    "            this_label = locator[1:]\n",
    "            labels = series_labels[locator[0].casefold()]['item_label'].array\n",
    "            if this_label.isdigit():\n",
    "                if not str(int(this_label)) in labels:\n",
    "                    raise ValueError('numerical label error')\n",
    "                    return False\n",
    "            elif this_label not in labels:\n",
    "                print(locator)\n",
    "                print(labels)\n",
    "                raise ValueError('alphanumeric label error')\n",
    "                return False\n",
    "\n",
    "    if len(r) == 4:\n",
    "        all1 = ['all', '1']\n",
    "        specs = r[3].split(':')\n",
    "        if (specs[0] not in all1) or (specs[1] not in all1):\n",
    "            raise ValueError('all or 1 error')\n",
    "            return False\n",
    "\n",
    "        error_text = ('relation {} includes a list specification for an '\n",
    "                      'item_label that has no list_delimiter in the label'\n",
    "                      'map.').format(r)\n",
    "\n",
    "        for i in [0, 1]:\n",
    "            locator = r[i]\n",
    "            if (r[3] == '1:1') or (specs[i] == 'all'):\n",
    "                if (len(locator) == 1):\n",
    "                    raise ValueError('list with no item locator')\n",
    "                    return False\n",
    "                labels = series_labels[locator[0].casefold()]\n",
    "                has_delimiter = pd.Series(labels['list_delimiter'].notna(),\n",
    "                                          index=labels['item_label'])\n",
    "                if not has_delimiter[locator[1:]]:\n",
    "                    raise ValueError(error_text)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def _escape_regex_metachars(s):\n",
    "    s = s.replace( \"\\\\\", \"\\\\\\\\\")\n",
    "    metachars = '.^$*+?{}[]|()'\n",
    "    for metachar in [c for c in metachars if c in s]:\n",
    "        s = s.replace(metachar, f'\\\\{metachar}')\n",
    "    return s\n",
    "\n",
    "\n",
    "def _strip_csv_comments(column, pattern):\n",
    "\n",
    "    column = column.str.split(pat=pattern, expand=True)\n",
    "    return column[0]\n",
    "\n",
    "\n",
    "def _replace_escaped_comment_chars(column, comment_char, pattern):\n",
    "\n",
    "    return column.replace(to_replace=pattern,\n",
    "                          value=comment_char,\n",
    "                          regex=True)\n",
    "\n",
    "\n",
    "def _normalize_shorthand_input(df,\n",
    "                               comment_char,\n",
    "                               fill_cols=(),\n",
    "                               drop_na=()):\n",
    "    '''\n",
    "    THIS FUNCTION MUTATES ITS FIRST ARGUMENT\n",
    "    '''\n",
    "    \n",
    "    if not iterable_not_string(fill_cols):\n",
    "        fill_cols = [fill_cols]\n",
    "    if not iterable_not_string(drop_na):\n",
    "        drop_na = [drop_na]\n",
    "\n",
    "    required_cols = ['left_entry',\n",
    "                     'right_entry',\n",
    "                     'link_tags_or_override',\n",
    "                     'reference']\n",
    "    valid_columns = [(c in map(str.casefold, df.columns)) for c in required_cols]\n",
    "    if not all(valid_columns):\n",
    "        raise ValueError('shorthand csv files must have a header whose first '\n",
    "                         'four column labels must be (ignoring case and list '\n",
    "                         'order):\\n >> [\"left_entry\", \"right_entry\", '\n",
    "                         '\"link_tags_or_override\", \"reference\"]')\n",
    "    \n",
    "    comment_char = _escape_regex_metachars(comment_char)\n",
    "    unescaped_comment_regex = r\"(?<!\\\\)[{}]\".format(comment_char)\n",
    "    escaped_comment_regex = fr\"(\\\\{comment_char})\"\n",
    "\n",
    "    # mask off cells erroneously created from commas in comments\n",
    "    has_comment = df.apply(lambda x: x.str.contains(unescaped_comment_regex))\n",
    "    has_comment = has_comment.fillna(False)\n",
    "    commented_out = has_comment.where(has_comment).ffill(axis=1).fillna(False)\n",
    "    commented_out = commented_out ^ has_comment\n",
    "    #df = _set_StringDtype(df.mask(commented_out)))\n",
    "    # can't use pd.StringDtype() throughout because it currently doesn't allow\n",
    "    # construction with null types other than pd.NA. This will likely change\n",
    "    # soon (https://github.com/pandas-dev/pandas/pull/41412)\n",
    "    df = df.mask(commented_out)\n",
    "    \n",
    "    # split cells where comments start and take the uncommented part\n",
    "    has_comment = has_comment.any(axis=1)\n",
    "    df.loc[has_comment, :] = df.loc[has_comment].apply(_strip_csv_comments,\n",
    "                                                       args=(comment_char,))\n",
    "    \n",
    "    # drop rows that began with comments\n",
    "    df = df.mask(df=='')\n",
    "    df = df.dropna(how='all')\n",
    "\n",
    "    # replace escaped comment characters with bare comment characters\n",
    "    df = df.apply(_replace_escaped_comment_chars,\n",
    "                  args=(comment_char, escaped_comment_regex))\n",
    "    \n",
    "    # optionally forward fill missing values\n",
    "    for column in fill_cols:\n",
    "        df.loc[:, column] = df.loc[:, column].ffill()\n",
    "\n",
    "    # optionally drop lines missing values\n",
    "    df = df.dropna(subset=drop_na)\n",
    "   \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_single_value(df, column_label, none_ok=False, group_key=None):\n",
    "    \n",
    "    if group_key is None:\n",
    "        message_tail = 'in column {}'.format(column_label)\n",
    "    else:\n",
    "        message_tail = ('in column {}, group key {}'\n",
    "                     .format(column_label, group_key))\n",
    "\n",
    "    values = df[column_label].value_counts()\n",
    "    num_values = len(values)\n",
    "\n",
    "    if num_values == 1:\n",
    "        return values.index[0]\n",
    "    elif num_values > 1:\n",
    "        raise ValueError('found multiple values {}'\n",
    "                         .format(message_tail))\n",
    "    elif (num_values < 1):\n",
    "        if not none_ok:\n",
    "            raise ValueError('no values found {}'\n",
    "                             .format(message_tail))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def _validate_entry_prefix_group(group):\n",
    "\n",
    "    get_single_value(group,\n",
    "                     'entry_node_type',\n",
    "                     none_ok=True,\n",
    "                     group_key=group.name)\n",
    "\n",
    "    series_node_type_na = group['entry_node_type'].isna()\n",
    "    link_type_na = group['item_link_type'].isna()\n",
    "    \n",
    "    if series_node_type_na.all():\n",
    "        if (~link_type_na).any():\n",
    "            raise ValueError('Error parsing labels for entry_prefix {}. '\n",
    "                             'There are no values in column \"entry_node '\n",
    "                             'type\" but column \"item_link_type\" contains '\n",
    "                             'values.'.format(group.name))\n",
    "    if link_type_na.all():\n",
    "        if (~series_node_type_na).any():\n",
    "            raise ValueError('Error parsing labels for entry_prefix {}. '\n",
    "                             'There are no values in column \"item_link_type\" '\n",
    "                             'but column \"entry_node type\" contains '\n",
    "                             'values.'.format(group.name))\n",
    "\n",
    "    item_node_type_na = group['item_node_type'].isna()\n",
    "    if item_node_type_na.any():\n",
    "        if group.loc[item_node_type_na, 'item_link_type'].notna().any():\n",
    "            raise ValueError('Error parsing labels for entry_prefix {}. '\n",
    "                             'If any row has no value in column \"item_node_'\n",
    "                             'type\" then column \"item_link_type\" must also '\n",
    "                             'have no value in that row.'.format(group.name))\n",
    "\n",
    "    list_delimiter_not_na = group['list_delimiter'].notna()\n",
    "    if list_delimiter_not_na.any():\n",
    "        if group.loc[list_delimiter_not_na, 'item_node_type'].isna().any():\n",
    "            raise ValueError('Error parsing labels for entry_prefix {}. '\n",
    "                             'If any row has a value in column \"list_'\n",
    "                             'delimiter\" then column \"item_node_type\" must '\n",
    "                             'also contain a value in that row.'\n",
    "                             .format(group.name))\n",
    "\n",
    "    item_prefix_separator_not_na = group['item_prefix_separator'].notna()\n",
    "    if item_prefix_separator_not_na.any():\n",
    "        has_item_prefix = group.loc[item_prefix_separator_not_na]\n",
    "        if has_item_prefix.loc[:, 'item_prefixes'].isna().any():\n",
    "            raise ValueError('Error parsing labels for entry_prefix {}. '\n",
    "                             'If any row has a value in column \"item_prefix_'\n",
    "                             'delimiter\" then column \"item_prefixes\" '\n",
    "                             'must also contain a value in that row.'\n",
    "                             .format(group.name))\n",
    "        if has_item_prefix.loc[:, 'item_node_type'].notna().any():\n",
    "            raise ValueError('Error parsing labels for entry_prefix {}. '\n",
    "                             'If any row has a value in column \"item_prefix '\n",
    "                             'delimiter\" then column \"item_node_type\" '\n",
    "                             'cannot contain values in that row.'\n",
    "                             .format(group.name))\n",
    "        if has_item_prefix.loc[:, 'item_link_type'].notna().any():\n",
    "            raise ValueError('Error parsing labels for entry_prefix {}. '\n",
    "                             'If any row has a value in column \"item_prefix '\n",
    "                             'delimiter\" then column \"item_link_type\" '\n",
    "                             'cannot contain values in that row.'\n",
    "                             .format(group.name))\n",
    "\n",
    "        if group['item_label'].duplicated().any():\n",
    "            raise ValueError('Error parsing labels for entry_prefix {}. '\n",
    "                             'Rows with the same entry prefix must have '\n",
    "                             'different item labels.'\n",
    "                             .format(group.name))\n",
    "\n",
    "\n",
    "def validate_entry_rules(entry_rules):\n",
    "    get_single_value(entry_rules, 'item_separator')\n",
    "    get_single_value(entry_rules, 'default_entry_prefix')\n",
    "\n",
    "    if entry_rules['item_label'].isna().any():\n",
    "        raise ValueError('Error parsing entry rules. All rows must have a '\n",
    "                         'value in column \"item_label\"')\n",
    "\n",
    "    entry_rules.groupby('entry_prefix').apply(_validate_entry_prefix_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bibliograph.util import iterable_not_string\n",
    "'''\n",
    "def _expand_grouped_entries(group,\n",
    "                            entry_syntax,\n",
    "                            item_separator,\n",
    "                            default_prefix,\n",
    "                            na_values):\n",
    "\n",
    "    expanded = group.str.rstrip(item_separator)\n",
    "\n",
    "    if pd.isna(group.name):\n",
    "        grp_prefix = default_prefix\n",
    "        \n",
    "    else:\n",
    "        grp_prefix = group.name\n",
    "        slice_start = len(grp_prefix + item_separator)\n",
    "        expanded = expanded.str.slice(start=slice_start)\n",
    "    \n",
    "    unescaped_sep_regex = r\"(?<!\\\\)[{}]\".format(']['.join(item_separator))\n",
    "    escaped_sep_regex = fr\"(\\\\{item_separator})\"\n",
    "\n",
    "    expanded = expanded.str.split(pat=unescaped_sep_regex, expand=True)\n",
    "    expanded = expanded.replace(to_replace=escaped_sep_regex,\n",
    "                                value=item_separator,\n",
    "                                regex=True)\n",
    "    for na in na_values:\n",
    "        expanded = expanded.replace(to_replace=na, value=pd.NA)\n",
    "                                                      \n",
    "    expanded.columns = [str(c) for c in expanded.columns]\n",
    "\n",
    "    csv_rows = expanded.index.get_level_values(0)\n",
    "    csv_cols = expanded.index.get_level_values(1)\n",
    "    grp_pfix = [grp_prefix]*len(expanded)\n",
    "    expanded.index = pd.MultiIndex.from_arrays((csv_rows,\n",
    "                                                csv_cols,\n",
    "                                                grp_pfix))\n",
    "\n",
    "    entry_labels = entry_syntax.query('entry_prefix == @grp_prefix')\n",
    "\n",
    "    row_with_item_prefix = entry_labels['item_node_type'].isna()\n",
    "\n",
    "    if row_with_item_prefix.any():\n",
    "\n",
    "        prefixed_items = entry_labels.loc[row_with_item_prefix]\n",
    "        prefixed_item_labels = prefixed_items['item_label'].array\n",
    "        prefixes = entry_labels.loc[~entry_labels['item_label'].str.isdigit(),\n",
    "                                    'item_label']\n",
    "        \n",
    "        disagged = expanded[prefixed_item_labels].stack()\n",
    "        \n",
    "        def item_prefix_splitter(group):\n",
    "\n",
    "            item_label = str(group.name)\n",
    "            this_item = prefixed_items.query('item_label == @item_label')\n",
    "            separator = get_single_value(this_item, 'item_prefix_separator')\n",
    "            default_pfix = get_single_value(this_item, 'item_prefixes')\n",
    "            default_pfix = default_pfix.split()[0]\n",
    "\n",
    "            item_pfixes_w_sep = [p + separator for p in prefixes]\n",
    "            default_pfix = default_pfix + separator\n",
    "\n",
    "            prefixes_regex = '|'.join([f'^({p})' for p in item_pfixes_w_sep])\n",
    "            has_no_pfix = ~group.str.match(prefixes_regex)\n",
    "            pfixed = group.copy()\n",
    "            pfixed.loc[has_no_pfix] = pfixed.loc[has_no_pfix] \\\n",
    "                                            .apply(lambda x: default_pfix + x)\n",
    "                                 \n",
    "            return pfixed.str.split(separator, n=1, expand=True)\n",
    "\n",
    "        disagged = disagged.groupby(level=3).apply(item_prefix_splitter)\n",
    "        disagged.index = disagged.index \\\n",
    "                                 .set_levels([grp_prefix], level=2) \\\n",
    "                                 .droplevel(3)\n",
    "        disagged = disagged.pivot(columns=0)\n",
    "        disagged.columns = disagged.columns.get_level_values(1)\n",
    "        \n",
    "        unprefixed_item_labels = [label for label in entry_labels['item_label']\n",
    "                                  if label.isdigit()]\n",
    "        unprefixed_item_labels = [label for label in unprefixed_item_labels \n",
    "                                        if label in expanded.columns\n",
    "                                        and label not in prefixed_item_labels]\n",
    "\n",
    "        expanded = expanded[unprefixed_item_labels]\n",
    "        \n",
    "        return pd.concat([expanded, disagged], axis='columns')\n",
    "\n",
    "    else:\n",
    "        return expanded\n",
    "'''\n",
    "def item_prefix_splitter(item_grp, prefixed_items):\n",
    "    '''\n",
    "    Dear Future Me: This function was refactored to ignore an\n",
    "    argument \"expected_prefixes\" that was previously passed in.\n",
    "    If you run into trouble you might have to revisit that.\n",
    "    '''\n",
    "\n",
    "    # get the labels for the item corresponding to the current\n",
    "    # item_label = str(item_grp.name)\n",
    "    # this_item = prefixed_items.query('item_label == @item_label')\n",
    "    item_syntax = prefixed_items.query('item_label == @item_grp.name')\n",
    "    item_pfx_separator = get_single_value(\n",
    "        item_syntax,\n",
    "        'item_prefix_separator'\n",
    "    )\n",
    "    # default_pfix = util.get_single_value(item_syntax, 'item_prefixes')\n",
    "    # default_pfix = default_pfix.split()[0]\n",
    "    # Get the default prefix and the set of possible prefixes\n",
    "    # for this item from the entry syntax\n",
    "    expected_prefixes = get_single_value(\n",
    "        item_syntax,\n",
    "        'item_prefixes'\n",
    "    )\n",
    "    expected_prefixes = expected_prefixes.split()\n",
    "    default_pfix = expected_prefixes[0]\n",
    "\n",
    "    # add the item prefix separator onto the prefixes\n",
    "    expected_prefixes = [\n",
    "        p + item_pfx_separator for p in expected_prefixes\n",
    "    ]\n",
    "    default_pfix = default_pfix + item_pfx_separator\n",
    "\n",
    "    # make a regular expression that will match any of the\n",
    "    # possible prefixes for this item\n",
    "    prefixes_regex = '|'.join([f'^({p})' for p in expected_prefixes])\n",
    "    has_no_pfix = ~item_grp.str.match(prefixes_regex)\n",
    "\n",
    "    # Add the default prefix onto unprefixed instances of this\n",
    "    # item. Copy the group first so we don't mutate a grouped\n",
    "    # object during iteration.\n",
    "    pfixed = item_grp.copy()\n",
    "    pfixed.loc[has_no_pfix] = pfixed.loc[has_no_pfix] \\\n",
    "                                    .apply(lambda x: default_pfix + x)\n",
    "\n",
    "    # Split the group on the prefix separator and expand into a\n",
    "    # dataframe, then return the dataframe\n",
    "    return pfixed.str.split(item_pfx_separator, n=1, expand=True)\n",
    "\n",
    "\n",
    "def _expand_grouped_entries(entry_grp,\n",
    "                            entry_syntax,\n",
    "                            item_separator,\n",
    "                            default_prefix,\n",
    "                            na_values):\n",
    "    '''\n",
    "    Takes a pandas.Series of stacked strings representing shorthand\n",
    "    entries and expands them into their component items according to\n",
    "    rules defined by the entry syntax.\n",
    "\n",
    "    Helper function for parse_entries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entry_grp : pandas.Series\n",
    "        A group of entries generated by pandas.Series.groupby. This\n",
    "        series has a multiindex generated by pandas.DataFrame.stack\n",
    "\n",
    "    entry_syntax : pandas.DataFrame\n",
    "        A dataframe containing item node types and item labels for each\n",
    "        type of entry.\n",
    "\n",
    "    item_separator : str\n",
    "        A string separating items within an entry string.\n",
    "\n",
    "    default_prefix : str\n",
    "        Entries with no prefix will be interpreted as having the default\n",
    "        prefix.\n",
    "\n",
    "    na_values : list-like\n",
    "        Items within an entry whose values are in na_values will be\n",
    "        replaced with pandas.NA\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame the same length as the input with columns defined\n",
    "        by the item_label column in entry_syntax.\n",
    "    '''\n",
    "    # strip any trailing separators from each entry this operation\n",
    "    # creates a copy of the input group, so we're free to mutate the\n",
    "    # copy\n",
    "    expanded = entry_grp.str.rstrip(item_separator)\n",
    "\n",
    "    if pd.isna(entry_grp.name):\n",
    "        grp_prefix = default_prefix\n",
    "\n",
    "    else:\n",
    "        grp_prefix = entry_grp.name\n",
    "        slice_start = len(grp_prefix + item_separator)\n",
    "        expanded = expanded.str.slice(start=slice_start)\n",
    "\n",
    "    # regular expressions to locate bare and escaped item separators\n",
    "    unescaped_sep_regex = r\"(?<!\\\\)[{}]\".format(']['.join(item_separator))\n",
    "    escaped_sep_regex = fr\"(\\\\{item_separator})\"\n",
    "    # split on bare item separators and expand into a dataframe\n",
    "    expanded = expanded.str.split(pat=unescaped_sep_regex, expand=True)\n",
    "    # replace escaped item separators with the bare value\n",
    "    expanded = expanded.replace(to_replace=escaped_sep_regex,\n",
    "                                value=item_separator,\n",
    "                                regex=True)\n",
    "    # replace missing values with pd.NA\n",
    "    for na in na_values:\n",
    "        expanded = expanded.replace(to_replace=na, value=pd.NA)\n",
    "\n",
    "    # column values were integers after str.split(pat, expand=True)\n",
    "    # above but they need to be strings because string-valued item\n",
    "    # labels are allowed\n",
    "    expanded.columns = [str(c) for c in expanded.columns]\n",
    "\n",
    "    # the group has a multiindex whose values are rows and columns in\n",
    "    # the csv input file we're parsing\n",
    "    # csv_rows = expanded.index.get_level_values(0)\n",
    "    # csv_cols = expanded.index.get_level_values(1)\n",
    "    # grp_pfix = [grp_prefix]*len(expanded)\n",
    "    # expanded.index = pd.MultiIndex.from_arrays(\n",
    "    #     (csv_rows, csv_cols, grp_pfix)\n",
    "    # )\n",
    "\n",
    "    # group prefixes are required to sort out entry relations later, so\n",
    "    # add an index level for the group prefix\n",
    "    expanded['grp_prefix'] = [grp_prefix]*len(expanded)\n",
    "    expanded = expanded.set_index('grp_prefix', append=True)\n",
    "\n",
    "    # get item labels and node types for this entry prefix\n",
    "    item_labels = entry_syntax.query('entry_prefix == @grp_prefix')\n",
    "\n",
    "    # items with no node type in the entry syntax are prefixed to\n",
    "    # indicate which node type they correspond to\n",
    "    item_is_prefixed = item_labels['item_node_type'].isna()\n",
    "\n",
    "    if item_is_prefixed.any():\n",
    "\n",
    "        prefixed_items = item_labels.loc[item_is_prefixed]\n",
    "        labels_of_prefixed_items = prefixed_items['item_label'].array\n",
    "        # We can identify item prefixes because they are not allowed to\n",
    "        # be numeric strings. Item labels (as opposed to prefixes) must\n",
    "        # be numeric strings becase entries are string-valued delimited\n",
    "        # lists of items, so the item labels are positional indexes in\n",
    "        # the list.\n",
    "        # expected_prefixes = item_labels.loc[\n",
    "        #    ~item_labels['item_label'].str.isdigit(),\n",
    "        #    'item_label'\n",
    "        # ]\n",
    "\n",
    "        # stack the prefixed items into a series\n",
    "        disagged = expanded[labels_of_prefixed_items].stack()\n",
    "\n",
    "        # Split the prefixes off of the stacked items and expand into a\n",
    "        # dataframe\n",
    "        disagged = disagged.groupby(level=3).apply(\n",
    "            item_prefix_splitter,\n",
    "            prefixed_items\n",
    "        )\n",
    "\n",
    "        # disagged.index = disagged.index.set_levels([grp_prefix], level=2)\n",
    "        # drop the item labels from the multiindex so the disaggregated\n",
    "        # items align with the index of the entry group\n",
    "        disagged.index = disagged.index.droplevel(3)\n",
    "\n",
    "        # pivot the disaggregated items to create a dataframe with\n",
    "        # columns for each item prefix\n",
    "        disagged = disagged.pivot(columns=0)\n",
    "        disagged.columns = disagged.columns.get_level_values(1)\n",
    "\n",
    "        # unprefixed_item_labels = [label for label in item_labels['item_label']\n",
    "        #                           if label.isdigit()]\n",
    "        # get labels of items that are not prefixed and present in this\n",
    "        # dataset\n",
    "        unprefixed_item_labels = [\n",
    "            label for label in item_labels['item_label']\n",
    "            if label.isdigit()\n",
    "            and label in expanded.columns\n",
    "            and label not in labels_of_prefixed_items\n",
    "        ]\n",
    "\n",
    "        # select only the unprefixed item labels\n",
    "        expanded = expanded[unprefixed_item_labels]\n",
    "        # concatenate the unprefixed and prefixed items and return\n",
    "        return pd.concat([expanded, disagged], axis='columns')\n",
    "\n",
    "    else:\n",
    "        # there were no prefixed items, so return the expanded items\n",
    "        # directly\n",
    "        return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bibliograph.util import iterable_not_string\n",
    "\n",
    "def _set_StringDtype(df):\n",
    "    # can't use pd.StringDtype() throughout because it currently doesn't allow\n",
    "    # construction with null types other than pd.NA. This will likely change\n",
    "    # soon (https://github.com/pandas-dev/pandas/pull/41412)\n",
    "    df_cols = df.columns\n",
    "    return df.astype(dict(zip(df_cols, [pd.StringDtype()]*len(df_cols))))\n",
    "\n",
    "def parse_shorthand_entries(entries, entry_syntax, space_placeholder, na_values):\n",
    "\n",
    "    '''\n",
    "    THIS FUNCTION MUTATES ITS FIRST ARGUMENT\n",
    "    '''\n",
    "    space_placeholder = _escape_regex_metachars(space_placeholder)\n",
    "    if iterable_not_string(na_values):\n",
    "        na_values = [_escape_regex_metachars(v) for v in na_values]\n",
    "    else:\n",
    "        na_values = [_escape_regex_metachars(na_values)]\n",
    "\n",
    "    default_entry_prefix = get_single_value(entry_syntax, 'default_entry_prefix')\n",
    "    default_entry_prefix = _escape_regex_metachars(default_entry_prefix)\n",
    "    item_separator = get_single_value(entry_syntax, 'item_separator')\n",
    "    item_separator = _escape_regex_metachars(item_separator)\n",
    "\n",
    "    pfixes_w_sep = [p + item_separator \n",
    "                    for p in entry_syntax['entry_prefix'].drop_duplicates()]\n",
    "\n",
    "    # use a regex to split tags off the input strings\n",
    "    tag_sep_regex = r\"(?<!\\\\)[{}][ ]\".format(']['.join(item_separator))\n",
    "    entries = entries.str.split(pat=tag_sep_regex, expand=True)\n",
    "\n",
    "    if len(entries.columns) == 1:\n",
    "        entries = entries.rename(columns={0:'string'})\n",
    "        entries['node_tags'] = pd.NA\n",
    "    else:\n",
    "        entries = entries.rename(columns={0:'string', 1:'node_tags'})\n",
    "    \n",
    "    prefixes_regex = '|'.join([f'^({p})' for p in pfixes_w_sep])\n",
    "    has_prefix = entries['string'].str.match(prefixes_regex)\n",
    "    prefixes = entries.loc[has_prefix, 'string'].str.split(item_separator,\n",
    "                                                           expand=True)\n",
    "    entries.loc[has_prefix, 'entry_prefix'] = prefixes[0].array\n",
    "    \n",
    "    expanded = entries['string'].groupby(by=entries['entry_prefix'],\n",
    "                                         dropna=False,\n",
    "                                         group_keys=False)\n",
    "    expanded = expanded.apply(_expand_grouped_entries,\n",
    "                              entry_syntax,\n",
    "                              item_separator,\n",
    "                              default_entry_prefix,\n",
    "                              na_values)\n",
    "   \n",
    "    # join tag strings back onto values in the 'string' column to recover\n",
    "    # the original input strings\n",
    "    where_tags = entries['node_tags'].notna()\n",
    "    entries.loc[where_tags, 'string'] = entries.loc[where_tags, 'string'] \\\n",
    "                                        + item_separator + ' ' \\\n",
    "                                        + entries.loc[where_tags, 'node_tags']\n",
    "    \n",
    "    entry_node_types = entry_syntax.loc[:, ['entry_prefix', 'entry_node_type']]\n",
    "    entry_node_types = entry_node_types.drop_duplicates()\n",
    "    entry_node_types = pd.Series(entry_node_types['entry_node_type'].array,\n",
    "                                 index=entry_node_types['entry_prefix'].array)\n",
    "\n",
    "    entry_prefixes =  entries.loc[:, 'entry_prefix']\n",
    "    entry_prefixes.loc[entry_prefixes.isna()] = default_entry_prefix\n",
    "\n",
    "    entries['node_type'] = entry_node_types[entry_prefixes].array\n",
    "    #entries['node_type'] = entries['node_type'].astype(pd.StringDtype())\n",
    "    # can't use pd.StringDtype() throughout because it currently doesn't allow\n",
    "    # construction with null types other than pd.NA. This will likely change\n",
    "    # soon (https://github.com/pandas-dev/pandas/pull/41412)\n",
    "\n",
    "    entries = entries.set_index('entry_prefix', append=True, drop=True)\n",
    "    entries['item_label'] = pd.NA\n",
    "    entries = entries.set_index('item_label', append=True, drop=True)\n",
    "\n",
    "    expanded = expanded.mask(expanded=='', pd.NA)\n",
    "\n",
    "    plchldr_regex = r\"(?<!\\\\)({})\".format(space_placeholder)\n",
    "    esc_plchldr_regex = fr\"(\\\\{space_placeholder})\"\n",
    "    \n",
    "    #expanded = expanded.apply(lambda x: x.str.replace(pat=plchldr_regex, \n",
    "    #                                                  repl=' ',\n",
    "    #                                                 regex=True))\n",
    "    expanded = expanded.replace(to_replace=plchldr_regex,\n",
    "                                value=' ',\n",
    "                                regex=True)\n",
    "    #expanded = expanded.apply(lambda x: x.str.replace(pat=esc_plchldr_regex, \n",
    "    #                                                  repl=space_placeholder,\n",
    "    #                                                  regex=True))\n",
    "    expanded = expanded.replace(to_replace=esc_plchldr_regex,\n",
    "                                value=space_placeholder,\n",
    "                                regex=True)\n",
    "\n",
    "    expanded = expanded.stack()\n",
    "\n",
    "    item_label_idx = pd.MultiIndex.from_arrays((entry_syntax['entry_prefix'],\n",
    "                                                entry_syntax['item_label']))\n",
    "    item_types = pd.DataFrame({'node_type':entry_syntax['item_node_type'].array,\n",
    "                               'link_type':entry_syntax['item_link_type'].array},\n",
    "                              index=item_label_idx)\n",
    "\n",
    "\n",
    "    #expanded.name = 'string'\n",
    "    #expanded = pd.DataFrame(expanded)\n",
    "    #cols = ['node_type', 'item_link_type']\n",
    "    \n",
    "    #expanded[cols] = item_types.loc[expanded.index.droplevel([0,1])].to_numpy()\n",
    "    entry_item_idx = expanded.index.droplevel([0, 1])\n",
    "    item_types = item_types.loc[entry_item_idx].set_index(expanded.index)\n",
    "    expanded = pd.concat([expanded.rename('string'), item_types], axis=1)\n",
    "    #expanded = _set_StringDtype(expanded)\n",
    "\n",
    "    return pd.concat([expanded, entries]).sort_index().fillna(pd.NA)\n",
    "    #expanded.append(entries).sort_index().fillna(pd.NA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibliograph as bg\n",
    "\n",
    "string_id_dtype = pd.UInt32Dtype()\n",
    "\n",
    "def _create_id_map(domain, map_na = False, **kwargs):\n",
    "    '''\n",
    "    Maps distinct values in a domain to a range of integers.  Additional\n",
    "    keyword arguments are passed to the pandas.Series constructor when\n",
    "    the map series is created.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    domain : list-like (coercible to pandas.Series)\n",
    "        Arbitrary set of values to map. May contain duplicates.\n",
    "\n",
    "    map_na : bool, default False\n",
    "        Map nan values to an integer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series whose length is the number of distinct values in the \n",
    "        input domain.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> dom = ['a', 'a', 'b', pd.NA, 'f', 'b']\n",
    "    >>> _create_id_map(dom, dtype=pd.UInt32Dtype())\n",
    "\n",
    "    a    0\n",
    "    b    1\n",
    "    f    2\n",
    "    dtype: UInt32\n",
    "    \n",
    "    >>> _create_id_map(dom, map_na=True, dtype=pd.UInt32Dtype())\n",
    "    \n",
    "    a       0\n",
    "    b       1\n",
    "    <NA>    2\n",
    "    f       3\n",
    "    dtype: UInt32\n",
    "    '''\n",
    "    # check if domain object has a str attribute like a pandas.Series\n",
    "    # and convert if not\n",
    "    try:\n",
    "        assert domain.str\n",
    "        # make a copy so we can mutate one (potentially large) object\n",
    "        # instead of creating additional references\n",
    "        domain = domain.copy()\n",
    "    except AttributeError:\n",
    "        domain = pd.Series(domain)\n",
    "\n",
    "    if not map_na:\n",
    "        domain = domain.loc[~domain.isna()]\n",
    "        \n",
    "    value_is_distinct = ~domain.duplicated()\n",
    "    num_distinct = value_is_distinct.value_counts()[True]\n",
    "\n",
    "    id_map = pd.Series(\n",
    "        range(num_distinct), \n",
    "        index = domain.loc[value_is_distinct].array,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return id_map\n",
    "\n",
    "def _extend_id_map(domain,\n",
    "                   existing_domain,\n",
    "                   existing_range = None,\n",
    "                   map_na = False,\n",
    "                   **kwargs):\n",
    "    '''\n",
    "    Map distinct values in a domain which are not in an existing domain\n",
    "    to integers that do not overlap with an existing range. Additional\n",
    "    keyword arguments are passed to the pandas.Series constructor when\n",
    "    the map series is created.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    domain : list-like (coercible to pandas.Series)\n",
    "        Arbitrary set of values to map. May contain duplicates.\n",
    "\n",
    "    existing_domain : list-like\n",
    "        Set of values already present in a map from values to integers.\n",
    "\n",
    "    existing_range : list-like or None, default None\n",
    "        Range of integers already present in the range of a map. If\n",
    "        None, assume existing_domain.index contains the existing range.\n",
    "\n",
    "    map_na : bool, default False\n",
    "        Map nan values to an integer.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> existing_domain = pd.Series(['a', 'a', 'b', 'f', 'b'])\n",
    "    >>> new_domain = ['a', 'b', 'z', pd.NA]\n",
    "    >>> _extend_id_map(new_domain,\n",
    "                       existing_domain,\n",
    "                       dtype = pd.UInt16Dtype())\n",
    "\n",
    "    z    5\n",
    "    dtype: UInt16\n",
    "\n",
    "    >>> _extend_id_map(new_domain,\n",
    "                       existing_domain,\n",
    "                       dtype = pd.UInt16Dtype(),\n",
    "                       map_na = True)\n",
    "\n",
    "    z       5\n",
    "    <NA>    6\n",
    "    dtype: UInt16\n",
    "    '''\n",
    "    # check if domain object has a str attribute like a pandas.Series\n",
    "    # and convert if not\n",
    "    try:\n",
    "        assert domain.str\n",
    "        # make a copy so we can mutate one (potentially large) object\n",
    "        # instead of creating additional references\n",
    "        domain = domain.copy()\n",
    "    except AttributeError:\n",
    "        domain = pd.Series(domain)\n",
    "\n",
    "    if not map_na:\n",
    "        domain = domain.loc[~domain.isna()]\n",
    "\n",
    "    domain_is_new = ~domain.isin(existing_domain)\n",
    "\n",
    "    if domain_is_new.any():\n",
    "        domain = domain.loc[domain_is_new].drop_duplicates()\n",
    "\n",
    "        if existing_range is None:\n",
    "            new_ids = bg.non_intersecting_sequence(\n",
    "                len(domain),\n",
    "                existing_domain.index\n",
    "            )\n",
    "        else:\n",
    "            new_ids = bg.non_intersecting_sequence(\n",
    "                len(domain),\n",
    "                existing_range\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        domain = []\n",
    "        new_ids = []\n",
    "\n",
    "    return pd.Series(new_ids, index = domain, **kwargs)\n",
    "\n",
    "def _split_item_lists(item_group, entry_syntax, prefix_id_map, item_id_map):\n",
    "    '''\n",
    "    THIS FUNCTION MUTATES ITS FIRST ARGUMENT\n",
    "    '''\n",
    "\n",
    "    if pd.isna(item_group.name[1]):\n",
    "        return item_group\n",
    "    else:\n",
    "        item_label = item_group.name[1]\n",
    "\n",
    "    item_label = item_id_map.loc[item_id_map == item_label].index[0]\n",
    "    \n",
    "    if pd.isna(item_label):\n",
    "        return item_group\n",
    "    \n",
    "    prefix = item_group.name[0]\n",
    "    prefix = prefix_id_map.loc[prefix_id_map == prefix].index[0]\n",
    "    label_row = entry_syntax.query('entry_prefix == @prefix') \\\n",
    "                            .query('item_label == @item_label') \\\n",
    "                            .squeeze()\n",
    "\n",
    "    delimiter = label_row['list_delimiter']\n",
    "\n",
    "    if pd.notna(delimiter):\n",
    "        item_group.loc[:, 'string'] = item_group['string'].str.split(delimiter)\n",
    "        #item_group['string'] = item_group['string'].astype(pd.StringDtype())\n",
    "        # can't use pd.StringDtype() throughout because it currently doesn't \n",
    "        # allow construction with null types other than pd.NA. This will\n",
    "        # likely change soon (https://github.com/pandas-dev/pandas/pull/41412)\n",
    "        return item_group\n",
    "    else:\n",
    "        return item_group\n",
    "\n",
    "\n",
    "def _make_item_list_indexes(index_group):\n",
    "    grp_len = len(index_group)\n",
    "    if grp_len == 1:\n",
    "        list_position = pd.Series([pd.NA]*grp_len)\n",
    "    else:\n",
    "        list_position = pd.Series(range(grp_len))\n",
    "    return list_position.astype(type_id_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "type_id_dtype = pd.UInt8Dtype()\n",
    "string_id_dtype = pd.UInt32Dtype()\n",
    "\n",
    "entry_syntax = pd.read_csv('./bibliograph/resources/label_map_new.csv')\n",
    "entry_syntax = entry_syntax.query('context == \"manual\"')\n",
    "\n",
    "validate_entry_rules(entry_syntax)\n",
    "\n",
    "########################################\n",
    "# Load a shorthand file and normalize it\n",
    "########################################\n",
    "\n",
    "skiprows = 2\n",
    "comment_char = '#'\n",
    "fill_cols = 'left_entry'\n",
    "drop_na = 'right_entry'\n",
    "\n",
    "data = pd.read_csv('./bibliograph/test_data/manual_annotation.shnd',\n",
    "                   skiprows = skiprows,\n",
    "                   #header = \n",
    "                   #comment = comment_char,\n",
    "                   skipinitialspace = True)\n",
    "#data = _set_StringDtype(data)\n",
    "# can't use pd.StringDtype() throughout because it currently doesn't \n",
    "# allow construction with null types other than pd.NA. This will\n",
    "# likely change soon (https://github.com/pandas-dev/pandas/pull/41412)\n",
    "data = _normalize_shorthand_input(data,\n",
    "                                  comment_char=comment_char,\n",
    "                                  fill_cols=fill_cols,\n",
    "                                  drop_na=drop_na)\n",
    "\n",
    "# Get any metadata for links between entries\n",
    "entry_link_has_override_or_tags = data['link_tags_or_override'].notna()\n",
    "if entry_link_has_override_or_tags.any():\n",
    "    rows_w_link_override_or_tags = data.loc[entry_link_has_override_or_tags] \\\n",
    "                                       .index\n",
    "\n",
    "#has_three_entries = data['reference'].notna()\n",
    "#if has_three_entries.any():\n",
    "#    rows_w_three_entries = data.loc[has_three_entries].index\n",
    "\n",
    "###################################################\n",
    "# Stack shorthand entries and delete original input\n",
    "###################################################\n",
    "\n",
    "# replace text column labels with integers so we compute on integer\n",
    "# indexes\n",
    "csv_column_id_map = _create_id_map(list(data.columns), dtype = pd.UInt8Dtype())\n",
    "data.columns = csv_column_id_map.array    \n",
    "\n",
    "entry_cols = csv_column_id_map[['left_entry', 'right_entry', 'reference']]\n",
    "stacked_entries = data[entry_cols].stack()\n",
    "del(data)\n",
    "\n",
    "###################################################\n",
    "# Parse distinct shorthand entry strings\n",
    "###################################################\n",
    "\n",
    "# Store the csv row indexes for duplicates so we can reconstruct assertions\n",
    "# about entries later\n",
    "\n",
    "entry_is_duplicated = stacked_entries.duplicated()\n",
    "\n",
    "if entry_is_duplicated.any():\n",
    "\n",
    "    distinct_entries = stacked_entries.loc[~entry_is_duplicated, :]\n",
    "\n",
    "    dplct_entries = stacked_entries.loc[entry_is_duplicated, :]\n",
    "\n",
    "    distinct_map = pd.Series(\n",
    "        distinct_entries.index.to_flat_index(),\n",
    "        index = distinct_entries.array\n",
    "    )\n",
    "    dplct_entries = dplct_entries.map(distinct_map)\n",
    "    dplct_entries = pd.DataFrame(\n",
    "        tuple(dplct_entries.array),\n",
    "        columns = ['string_csv_row', 'string_csv_col'],\n",
    "        index = dplct_entries.index\n",
    "    )\n",
    "    \n",
    "    del(distinct_map)\n",
    "\n",
    "    dplct_entries = dplct_entries.reset_index()\n",
    "    dplct_entries = dplct_entries.rename(\n",
    "        columns = {'level_0':'entry_csv_row',\n",
    "                   'level_1':'entry_csv_col'}\n",
    "    )\n",
    "\n",
    "    dplct_entries = dplct_entries.astype({\n",
    "        'entry_csv_row': string_id_dtype,\n",
    "        'entry_csv_col': type_id_dtype,\n",
    "        'string_csv_row': string_id_dtype,\n",
    "        'string_csv_col': type_id_dtype\n",
    "    })\n",
    "\n",
    "else:\n",
    "\n",
    "    distinct_entries = stacked_entries\n",
    "\n",
    "del(stacked_entries)\n",
    "\n",
    "distinct_entries = distinct_entries.str.strip()\n",
    "\n",
    "parsed_shnd = parse_shorthand_entries(\n",
    "    distinct_entries,\n",
    "    entry_syntax,\n",
    "    space_placeholder='|',\n",
    "    na_values='x'\n",
    ")\n",
    "parsed_shnd = parsed_shnd.reset_index()\n",
    "parsed_shnd = parsed_shnd.rename(\n",
    "    columns={'level_0':'csv_row',\n",
    "             'level_1':'csv_col',\n",
    "             'grp_prefix':'entry_prefix',\n",
    "             'level_3':'item_label'}\n",
    ")\n",
    "\n",
    "csv_idx_dtypes = {'csv_row':string_id_dtype,\n",
    "                  'csv_col':pd.UInt8Dtype()}\n",
    "parsed_shnd = parsed_shnd.astype(csv_idx_dtypes)\n",
    "#for c in ['entry_prefix', 'item_label']:\n",
    "#    parsed_shnd[c] = parsed_shnd[c].astype(pd.StringDtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csv_row</th>\n",
       "      <th>csv_col</th>\n",
       "      <th>entry_prefix</th>\n",
       "      <th>item_label</th>\n",
       "      <th>string</th>\n",
       "      <th>node_type</th>\n",
       "      <th>link_type</th>\n",
       "      <th>node_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>0</td>\n",
       "      <td>asmith_bwu</td>\n",
       "      <td>actors</td>\n",
       "      <td>author</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>1</td>\n",
       "      <td>1999</td>\n",
       "      <td>dates</td>\n",
       "      <td>published</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>works</td>\n",
       "      <td>volume</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>4</td>\n",
       "      <td>803</td>\n",
       "      <td>works</td>\n",
       "      <td>page</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>5</td>\n",
       "      <td>xxx</td>\n",
       "      <td>identifiers</td>\n",
       "      <td>doi</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>s</td>\n",
       "      <td>bams</td>\n",
       "      <td>works</td>\n",
       "      <td>supertitle</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>asmith_bwu__1999__bams__101__803__xxx</td>\n",
       "      <td>works</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>0</td>\n",
       "      <td>asmith_bwu</td>\n",
       "      <td>actors</td>\n",
       "      <td>author</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>1</td>\n",
       "      <td>1998</td>\n",
       "      <td>dates</td>\n",
       "      <td>published</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>works</td>\n",
       "      <td>volume</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>works</td>\n",
       "      <td>page</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>5</td>\n",
       "      <td>yyy</td>\n",
       "      <td>identifiers</td>\n",
       "      <td>doi</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>s</td>\n",
       "      <td>bams</td>\n",
       "      <td>works</td>\n",
       "      <td>supertitle</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>asmith_bwu__1998__bams__100__42__yyy</td>\n",
       "      <td>works</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>0</td>\n",
       "      <td>bjones</td>\n",
       "      <td>actors</td>\n",
       "      <td>author</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>1</td>\n",
       "      <td>1975</td>\n",
       "      <td>dates</td>\n",
       "      <td>published</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>works</td>\n",
       "      <td>volume</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>works</td>\n",
       "      <td>page</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>s</td>\n",
       "      <td>jats</td>\n",
       "      <td>works</td>\n",
       "      <td>supertitle</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bjones__1975__jats__90__1__ bjones1975_is_tagg...</td>\n",
       "      <td>works</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>bjones1975_is_tagged with some_stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>0</td>\n",
       "      <td>bwu</td>\n",
       "      <td>actors</td>\n",
       "      <td>author</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>1</td>\n",
       "      <td>1989</td>\n",
       "      <td>dates</td>\n",
       "      <td>published</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>works</td>\n",
       "      <td>page</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>t</td>\n",
       "      <td>long title</td>\n",
       "      <td>works</td>\n",
       "      <td>title</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bwu__1989__t_long|title__x__80</td>\n",
       "      <td>works</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>0</td>\n",
       "      <td>Some Author</td>\n",
       "      <td>actors</td>\n",
       "      <td>author</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>1</td>\n",
       "      <td>1989</td>\n",
       "      <td>dates</td>\n",
       "      <td>published</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>t</td>\n",
       "      <td>Title With #</td>\n",
       "      <td>works</td>\n",
       "      <td>title</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some Author__1989__t_Title With #__x__x</td>\n",
       "      <td>works</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>fun</td>\n",
       "      <td>0</td>\n",
       "      <td>nasa</td>\n",
       "      <td>actors</td>\n",
       "      <td>funder</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>fun</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA grant 12345-6789</td>\n",
       "      <td>agreements</td>\n",
       "      <td>title</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>fun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fun__nasa__NASA|grant|12345-6789</td>\n",
       "      <td>agreements</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>ack</td>\n",
       "      <td>0</td>\n",
       "      <td>margaret lemone_some other person</td>\n",
       "      <td>actors</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>ack</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ack__margaret|lemone_some|other|person</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>org</td>\n",
       "      <td>0</td>\n",
       "      <td>nasa_university of chicago</td>\n",
       "      <td>actors</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>org__nasa_university|of|chicago</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>qte</td>\n",
       "      <td>0</td>\n",
       "      <td>super great quote, right?</td>\n",
       "      <td>quote</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>qte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qte__super great quote, right?__</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>qte</td>\n",
       "      <td>0</td>\n",
       "      <td>This is the abstract of the article. We did st...</td>\n",
       "      <td>quote</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>qte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qte__This is the abstract of the article. We d...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>not</td>\n",
       "      <td>0</td>\n",
       "      <td>this is an article I made up for testing</td>\n",
       "      <td>note</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>not__this is an article I made up for testing_...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>some note tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>not</td>\n",
       "      <td>0</td>\n",
       "      <td>here's a note with an escaped__item separator ...</td>\n",
       "      <td>note</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>not__here's a note with an escaped\\__item sepa...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>1</td>\n",
       "      <td>weirdoPrefix</td>\n",
       "      <td>dates</td>\n",
       "      <td>published</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>s</td>\n",
       "      <td>some_string_representation</td>\n",
       "      <td>works</td>\n",
       "      <td>supertitle</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>wrk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>__weirdoPrefix__some_string_representation</td>\n",
       "      <td>works</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>wrk</td>\n",
       "      <td>0</td>\n",
       "      <td>an author</td>\n",
       "      <td>actors</td>\n",
       "      <td>author</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>wrk</td>\n",
       "      <td>1</td>\n",
       "      <td>2199</td>\n",
       "      <td>dates</td>\n",
       "      <td>published</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>wrk</td>\n",
       "      <td>s</td>\n",
       "      <td>some other source</td>\n",
       "      <td>works</td>\n",
       "      <td>supertitle</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>wrk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>an|author__2199__some|other|source__x__x</td>\n",
       "      <td>works</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>0</td>\n",
       "      <td>asmith_bwu</td>\n",
       "      <td>actors</td>\n",
       "      <td>author</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>dates</td>\n",
       "      <td>published</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>works</td>\n",
       "      <td>volume</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>works</td>\n",
       "      <td>page</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>5</td>\n",
       "      <td>zzz</td>\n",
       "      <td>identifiers</td>\n",
       "      <td>doi</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>s</td>\n",
       "      <td>bams</td>\n",
       "      <td>works</td>\n",
       "      <td>supertitle</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>wrk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>asmith_bwu__2008__bams__110__1__zzz</td>\n",
       "      <td>works</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    csv_row  csv_col entry_prefix item_label  \\\n",
       "0         1        0          wrk          0   \n",
       "1         1        0          wrk          1   \n",
       "2         1        0          wrk          3   \n",
       "3         1        0          wrk          4   \n",
       "4         1        0          wrk          5   \n",
       "5         1        0          wrk          s   \n",
       "6         1        0          wrk        NaN   \n",
       "7         1        1          wrk          0   \n",
       "8         1        1          wrk          1   \n",
       "9         1        1          wrk          3   \n",
       "10        1        1          wrk          4   \n",
       "11        1        1          wrk          5   \n",
       "12        1        1          wrk          s   \n",
       "13        1        1          wrk        NaN   \n",
       "14        2        1          wrk          0   \n",
       "15        2        1          wrk          1   \n",
       "16        2        1          wrk          3   \n",
       "17        2        1          wrk          4   \n",
       "18        2        1          wrk          s   \n",
       "19        2        1          wrk        NaN   \n",
       "20        3        1          wrk          0   \n",
       "21        3        1          wrk          1   \n",
       "22        3        1          wrk          4   \n",
       "23        3        1          wrk          t   \n",
       "24        3        1          wrk        NaN   \n",
       "25        4        1          wrk          0   \n",
       "26        4        1          wrk          1   \n",
       "27        4        1          wrk          t   \n",
       "28        4        1          wrk        NaN   \n",
       "29        5        1          fun          0   \n",
       "30        5        1          fun          1   \n",
       "31        5        1          fun        NaN   \n",
       "32        6        1          ack          0   \n",
       "33        6        1          ack        NaN   \n",
       "34        7        1          org          0   \n",
       "35        7        1          org        NaN   \n",
       "36        8        1          qte          0   \n",
       "37        8        1          qte        NaN   \n",
       "38        9        1          qte          0   \n",
       "39        9        1          qte        NaN   \n",
       "40       10        1          not          0   \n",
       "41       10        1          not        NaN   \n",
       "42       11        1          not          0   \n",
       "43       11        1          not        NaN   \n",
       "44       12        1          wrk          1   \n",
       "45       12        1          wrk          s   \n",
       "46       12        1          wrk        NaN   \n",
       "47       12        3          wrk          0   \n",
       "48       12        3          wrk          1   \n",
       "49       12        3          wrk          s   \n",
       "50       12        3          wrk        NaN   \n",
       "51       14        0          wrk          0   \n",
       "52       14        0          wrk          1   \n",
       "53       14        0          wrk          3   \n",
       "54       14        0          wrk          4   \n",
       "55       14        0          wrk          5   \n",
       "56       14        0          wrk          s   \n",
       "57       14        0          wrk        NaN   \n",
       "\n",
       "                                               string    node_type  \\\n",
       "0                                          asmith_bwu       actors   \n",
       "1                                                1999        dates   \n",
       "2                                                 101        works   \n",
       "3                                                 803        works   \n",
       "4                                                 xxx  identifiers   \n",
       "5                                                bams        works   \n",
       "6               asmith_bwu__1999__bams__101__803__xxx        works   \n",
       "7                                          asmith_bwu       actors   \n",
       "8                                                1998        dates   \n",
       "9                                                 100        works   \n",
       "10                                                 42        works   \n",
       "11                                                yyy  identifiers   \n",
       "12                                               bams        works   \n",
       "13               asmith_bwu__1998__bams__100__42__yyy        works   \n",
       "14                                             bjones       actors   \n",
       "15                                               1975        dates   \n",
       "16                                                 90        works   \n",
       "17                                                  1        works   \n",
       "18                                               jats        works   \n",
       "19  bjones__1975__jats__90__1__ bjones1975_is_tagg...        works   \n",
       "20                                                bwu       actors   \n",
       "21                                               1989        dates   \n",
       "22                                                 80        works   \n",
       "23                                         long title        works   \n",
       "24                     bwu__1989__t_long|title__x__80        works   \n",
       "25                                        Some Author       actors   \n",
       "26                                               1989        dates   \n",
       "27                                       Title With #        works   \n",
       "28            Some Author__1989__t_Title With #__x__x        works   \n",
       "29                                               nasa       actors   \n",
       "30                              NASA grant 12345-6789   agreements   \n",
       "31                   fun__nasa__NASA|grant|12345-6789   agreements   \n",
       "32                  margaret lemone_some other person       actors   \n",
       "33             ack__margaret|lemone_some|other|person         <NA>   \n",
       "34                         nasa_university of chicago       actors   \n",
       "35                    org__nasa_university|of|chicago         <NA>   \n",
       "36                          super great quote, right?        quote   \n",
       "37                   qte__super great quote, right?__         <NA>   \n",
       "38  This is the abstract of the article. We did st...        quote   \n",
       "39  qte__This is the abstract of the article. We d...         <NA>   \n",
       "40           this is an article I made up for testing         note   \n",
       "41  not__this is an article I made up for testing_...         <NA>   \n",
       "42  here's a note with an escaped__item separator ...         note   \n",
       "43  not__here's a note with an escaped\\__item sepa...         <NA>   \n",
       "44                                       weirdoPrefix        dates   \n",
       "45                         some_string_representation        works   \n",
       "46         __weirdoPrefix__some_string_representation        works   \n",
       "47                                          an author       actors   \n",
       "48                                               2199        dates   \n",
       "49                                  some other source        works   \n",
       "50           an|author__2199__some|other|source__x__x        works   \n",
       "51                                         asmith_bwu       actors   \n",
       "52                                               2008        dates   \n",
       "53                                                110        works   \n",
       "54                                                  1        works   \n",
       "55                                                zzz  identifiers   \n",
       "56                                               bams        works   \n",
       "57                asmith_bwu__2008__bams__110__1__zzz        works   \n",
       "\n",
       "     link_type                             node_tags  \n",
       "0       author                                  <NA>  \n",
       "1    published                                  <NA>  \n",
       "2       volume                                  <NA>  \n",
       "3         page                                  <NA>  \n",
       "4          doi                                  <NA>  \n",
       "5   supertitle                                  <NA>  \n",
       "6         <NA>                                  <NA>  \n",
       "7       author                                  <NA>  \n",
       "8    published                                  <NA>  \n",
       "9       volume                                  <NA>  \n",
       "10        page                                  <NA>  \n",
       "11         doi                                  <NA>  \n",
       "12  supertitle                                  <NA>  \n",
       "13        <NA>                                  <NA>  \n",
       "14      author                                  <NA>  \n",
       "15   published                                  <NA>  \n",
       "16      volume                                  <NA>  \n",
       "17        page                                  <NA>  \n",
       "18  supertitle                                  <NA>  \n",
       "19        <NA>  bjones1975_is_tagged with some_stuff  \n",
       "20      author                                  <NA>  \n",
       "21   published                                  <NA>  \n",
       "22        page                                  <NA>  \n",
       "23       title                                  <NA>  \n",
       "24        <NA>                                  <NA>  \n",
       "25      author                                  <NA>  \n",
       "26   published                                  <NA>  \n",
       "27       title                                  <NA>  \n",
       "28        <NA>                                  <NA>  \n",
       "29      funder                                  <NA>  \n",
       "30       title                                  <NA>  \n",
       "31        <NA>                                  <NA>  \n",
       "32        <NA>                                  <NA>  \n",
       "33        <NA>                                  <NA>  \n",
       "34        <NA>                                  <NA>  \n",
       "35        <NA>                                  <NA>  \n",
       "36        <NA>                                  <NA>  \n",
       "37        <NA>                                  <NA>  \n",
       "38        <NA>                                  <NA>  \n",
       "39        <NA>                                  <NA>  \n",
       "40        <NA>                                  <NA>  \n",
       "41        <NA>                        some note tags  \n",
       "42        <NA>                                  <NA>  \n",
       "43        <NA>                                  <NA>  \n",
       "44   published                                  <NA>  \n",
       "45  supertitle                                  <NA>  \n",
       "46        <NA>                                  <NA>  \n",
       "47      author                                  <NA>  \n",
       "48   published                                  <NA>  \n",
       "49  supertitle                                  <NA>  \n",
       "50        <NA>                                  <NA>  \n",
       "51      author                                  <NA>  \n",
       "52   published                                  <NA>  \n",
       "53      volume                                  <NA>  \n",
       "54        page                                  <NA>  \n",
       "55         doi                                  <NA>  \n",
       "56  supertitle                                  <NA>  \n",
       "57        <NA>                                  <NA>  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_shnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   link_type_id   link_type\n",
      "0             0      author\n",
      "1             1   published\n",
      "2             2      volume\n",
      "3             3        page\n",
      "4             4         doi\n",
      "5             5  supertitle\n",
      "6             6       title\n",
      "7             7      funder\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csv_row</th>\n",
       "      <th>csv_col</th>\n",
       "      <th>entry_prefix</th>\n",
       "      <th>item_label</th>\n",
       "      <th>item_list_position</th>\n",
       "      <th>string_id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>link_type_id</th>\n",
       "      <th>node_type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    csv_row  csv_col  entry_prefix  item_label  item_list_position  string_id  \\\n",
       "0         1        0             0           0                   0          0   \n",
       "1         1        0             0           0                   1          1   \n",
       "2         1        0             0           1                <NA>          2   \n",
       "3         1        0             0           2                <NA>          3   \n",
       "4         1        0             0           3                <NA>          4   \n",
       "5         1        0             0           4                <NA>          5   \n",
       "6         1        0             0           5                <NA>          6   \n",
       "7         1        0             0        <NA>                <NA>          7   \n",
       "8         1        1             0           0                   0          0   \n",
       "9         1        1             0           0                   1          1   \n",
       "10        1        1             0           1                <NA>          8   \n",
       "11        1        1             0           2                <NA>          9   \n",
       "12        1        1             0           3                <NA>         10   \n",
       "13        1        1             0           4                <NA>         11   \n",
       "14        1        1             0           5                <NA>          6   \n",
       "15        1        1             0        <NA>                <NA>         12   \n",
       "16        2        1             0           0                <NA>         13   \n",
       "17        2        1             0           1                <NA>         14   \n",
       "18        2        1             0           2                <NA>         15   \n",
       "19        2        1             0           3                <NA>         16   \n",
       "20        2        1             0           5                <NA>         17   \n",
       "21        2        1             0        <NA>                <NA>         18   \n",
       "22        3        1             0           0                <NA>          1   \n",
       "23        3        1             0           1                <NA>         19   \n",
       "24        3        1             0           3                <NA>         20   \n",
       "25        3        1             0           6                <NA>         21   \n",
       "26        3        1             0        <NA>                <NA>         22   \n",
       "27        4        1             0           0                <NA>         23   \n",
       "28        4        1             0           1                <NA>         19   \n",
       "29        4        1             0           6                <NA>         24   \n",
       "\n",
       "    node_id  link_type_id  node_type_id  \n",
       "0         0             0             0  \n",
       "1         1             0             0  \n",
       "2         2             1             1  \n",
       "3         3             2             2  \n",
       "4         4             3             2  \n",
       "5         5             4             3  \n",
       "6         6             5             2  \n",
       "7         7          <NA>             2  \n",
       "8         0             0             0  \n",
       "9         1             0             0  \n",
       "10        8             1             1  \n",
       "11        9             2             2  \n",
       "12       10             3             2  \n",
       "13       11             4             3  \n",
       "14        6             5             2  \n",
       "15       12          <NA>             2  \n",
       "16       13             0             0  \n",
       "17       14             1             1  \n",
       "18       15             2             2  \n",
       "19       16             3             2  \n",
       "20       17             5             2  \n",
       "21       18          <NA>             2  \n",
       "22        1             0             0  \n",
       "23       19             1             1  \n",
       "24       20             3             2  \n",
       "25       21             6             2  \n",
       "26       22          <NA>             2  \n",
       "27       23             0             0  \n",
       "28       19             1             1  \n",
       "29       24             6             2  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################\n",
    "# Map integer IDs for entry prefixes and item labels\n",
    "####################################################\n",
    "entry_pfx_id_map = _create_id_map(\n",
    "    parsed_shnd['entry_prefix'],\n",
    "    dtype = type_id_dtype\n",
    ")\n",
    "\n",
    "parsed_shnd['entry_prefix'] = parsed_shnd['entry_prefix'].map(entry_pfx_id_map)\n",
    "\n",
    "item_label_id_map = _create_id_map(\n",
    "    parsed_shnd['item_label'],\n",
    "    dtype = type_id_dtype\n",
    ")\n",
    "\n",
    "parsed_shnd['item_label'] = parsed_shnd['item_label'].map(item_label_id_map)\n",
    "\n",
    "#######################################\n",
    "# Split strings that are lists of items\n",
    "#######################################\n",
    "parsed_shnd = parsed_shnd.groupby(by=['entry_prefix', 'item_label'],\n",
    "                                  dropna=False)\n",
    "parsed_shnd = parsed_shnd.apply(_split_item_lists,\n",
    "                                entry_syntax,\n",
    "                                entry_pfx_id_map,\n",
    "                                item_label_id_map)\n",
    "\n",
    "parsed_shnd = parsed_shnd.explode('string')\n",
    "\n",
    "item_list_position = pd.Series(parsed_shnd.index).groupby(parsed_shnd.index)\n",
    "item_list_position = item_list_position.apply(_make_item_list_indexes)\n",
    "parsed_shnd['item_list_position'] = item_list_position.array\n",
    "\n",
    "parsed_shnd = parsed_shnd.reset_index(drop=True)\n",
    "\n",
    "######################################################\n",
    "# Map string and node IDs, separate strings from nodes\n",
    "######################################################\n",
    "string_id_map = _create_id_map(parsed_shnd['string'], dtype = string_id_dtype)\n",
    "parsed_shnd['string_id'] = parsed_shnd['string'].map(string_id_map)\n",
    "\n",
    "del(string_id_map)\n",
    "\n",
    "# assume distinct strings are distinct nodes\n",
    "parsed_shnd['node_id'] = parsed_shnd['string_id'].array\n",
    "\n",
    "strings = parsed_shnd.loc[:, ['string_id', 'node_id', 'string']]\n",
    "strings = strings.drop_duplicates()\n",
    "\n",
    "parsed_shnd = parsed_shnd.drop('string', axis=1)\n",
    "\n",
    "################\n",
    "# Get link types\n",
    "################\n",
    "link_types = _create_id_map(\n",
    "    parsed_shnd['link_type'],\n",
    "    dtype = type_id_dtype\n",
    ")\n",
    "\n",
    "parsed_shnd['link_type_id'] = parsed_shnd['link_type'].map(link_types)\n",
    "parsed_shnd = parsed_shnd.drop('link_type', axis=1)\n",
    "\n",
    "link_types = link_types.reset_index()\n",
    "link_types = link_types[[0, 'index']]\n",
    "link_types = link_types.rename(\n",
    "    columns = {0: 'link_type_id', 'index': 'link_type'}\n",
    ")\n",
    "\n",
    "################\n",
    "# Get node types\n",
    "################\n",
    "node_types = _create_id_map(parsed_shnd['node_type'], dtype = type_id_dtype)\n",
    "\n",
    "parsed_shnd['node_type_id'] = parsed_shnd['node_type'].map(node_types)\n",
    "parsed_shnd = parsed_shnd.drop('node_type', axis=1)\n",
    "\n",
    "node_types = node_types.reset_index()\n",
    "node_types = node_types[[0, 'index']]\n",
    "node_types = node_types.rename(\n",
    "    columns = {0: 'node_type_id', 'index': 'node_type'}\n",
    ")\n",
    "\n",
    "################\n",
    "# Get node tags\n",
    "################\n",
    "node_tags = parsed_shnd.loc[parsed_shnd['node_tags'].notna(),\n",
    "                            ['node_id', 'node_tags']]\n",
    "\n",
    "parsed_shnd = parsed_shnd.drop('node_tags', axis=1)\n",
    "\n",
    "parsed_shnd.loc[~parsed_shnd['link_type_id'].isna()]\n",
    "print(link_types)\n",
    "parsed_shnd.iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a      g\n",
      "0  1  alpha\n",
      "1  2   beta\n",
      "2  3  gamma\n",
      "0  4  delta\n",
      "   a      g\n",
      "0  1  alpha\n",
      "1  2   beta\n",
      "2  3  gamma\n",
      "0  4  delta\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>beta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>gamma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>delta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a      g\n",
       "0  1  alpha\n",
       "1  2   beta\n",
       "2  3  gamma\n",
       "0  4  delta"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.DataFrame({'a': [1, 2, 3], 'g': ['alpha', 'beta', 'gamma']})\n",
    "b = pd.Series({'a': 4, 'g': 'delta'})\n",
    "print(pd.concat([a, pd.DataFrame(b).T]))\n",
    "\n",
    "b = pd.DataFrame({'a': [4], 'g': ['delta']})\n",
    "print(pd.concat([a, b]))\n",
    "\n",
    "b = [a['a'].max() + 1, 'delta']\n",
    "b = pd.DataFrame(\n",
    "    dict(\n",
    "        zip(\n",
    "            a.columns,\n",
    "            [[e] for e in b]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "pd.concat([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_entry_prefix</th>\n",
       "      <th>right_entry_prefix</th>\n",
       "      <th>list_mode</th>\n",
       "      <th>link_type_id</th>\n",
       "      <th>src_csv_col</th>\n",
       "      <th>src_item_label</th>\n",
       "      <th>tgt_csv_col</th>\n",
       "      <th>tgt_item_label</th>\n",
       "      <th>ref_csv_col</th>\n",
       "      <th>ref_item_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>R</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1:1</td>\n",
       "      <td>9</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>m:m</td>\n",
       "      <td>7</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>m:m</td>\n",
       "      <td>7</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>m:m</td>\n",
       "      <td>10</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>L</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left_entry_prefix  right_entry_prefix list_mode  link_type_id src_csv_col  \\\n",
       "0                  0                   0       NaN             8           L   \n",
       "1                  0                   3       1:1             9           L   \n",
       "2                  0                   1       NaN             7           R   \n",
       "3                  0                   1       m:m             7           R   \n",
       "4                  0                   1       NaN             7           R   \n",
       "5                  0                   1       m:m             7           R   \n",
       "6                  0                   2       m:m            10           L   \n",
       "7                  0                   4       NaN            11           L   \n",
       "8                  0                   5       NaN            12           L   \n",
       "\n",
       "   src_item_label tgt_csv_col  tgt_item_label ref_csv_col  ref_item_label  \n",
       "0            <NA>           R            <NA>           L            <NA>  \n",
       "1               0           R               0           L            <NA>  \n",
       "2               0           L            <NA>           L            <NA>  \n",
       "3               0           L               0           L            <NA>  \n",
       "4               1           L            <NA>           L            <NA>  \n",
       "5               1           L               0           L            <NA>  \n",
       "6            <NA>           R               0           L            <NA>  \n",
       "7            <NA>           R               0           L            <NA>  \n",
       "8            <NA>           R               0         NaN            <NA>  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bibliograph as bg\n",
    "\n",
    "###############################################################\n",
    "# SETUP ENTRY RELATIONS\n",
    "###############################################################\n",
    "relations = pd.read_csv('./bibliograph/resources/entry_relations.csv',\n",
    "                        skipinitialspace=True)\n",
    "\n",
    "# TODO: implement relations file validation\n",
    "# _validate_entry_relations(relations)\n",
    "\n",
    "# Select relations with entry prefix pairs that exist in this data set\n",
    "prefix_cols = ['left_entry_prefix', 'right_entry_prefix']\n",
    "\n",
    "for column in prefix_cols:\n",
    "    relations = relations.loc[relations[column].isin(entry_pfx_id_map.index)]\n",
    "    relations[column] = relations[column].map(entry_pfx_id_map)\n",
    "\n",
    "# Get any new link types in these entry relations\n",
    "new_link_types = _extend_id_map(relations['link_type'],\n",
    "                                link_types['link_type'],\n",
    "                                link_types['link_type_id'],\n",
    "                                dtype = type_id_dtype)\n",
    "new_link_types = pd.DataFrame({'link_type_id':new_link_types.array,\n",
    "                               'link_type':new_link_types.index})\n",
    "link_types = pd.concat([link_types, new_link_types])\n",
    "\n",
    "relations['link_type_id'] = relations['link_type'].map(\n",
    "    pd.Series(\n",
    "        link_types['link_type_id'].array,\n",
    "        index = link_types['link_type']\n",
    "    )\n",
    ")\n",
    "\n",
    "relations = relations.drop('link_type', axis=1)\n",
    "\n",
    "# Parse the position codes from the entry relations\n",
    "def _make_node_locators(rltn_column):\n",
    "    position_indicator = relations[rltn_column]\n",
    "\n",
    "    csv_col = (position_indicator.str.slice(0, 1) == 'R')\n",
    "    csv_col = csv_col.mask(position_indicator.isna())\n",
    "\n",
    "    item_label = position_indicator.str.slice(1)\n",
    "    item_label = item_label.mask(item_label == '')\n",
    "\n",
    "    return csv_col, item_label\n",
    "\n",
    "columns_and_prefixes = {'source_position':'src_',\n",
    "                        'target_position':'tgt_',\n",
    "                        'dflt_ref_position':'ref_'}\n",
    "for column, prefix in columns_and_prefixes.items():\n",
    "    position_indicator = relations[column]\n",
    "    relations[prefix + 'csv_col'] = position_indicator.str.slice(0, 1).array\n",
    "    item_label = position_indicator.str.slice(1)\n",
    "    item_label = item_label.mask(item_label == '')\n",
    "    relations[prefix + 'item_label'] = item_label\n",
    "    relations = relations.drop(column, axis=1)\n",
    "\n",
    "# Validate item labels in the relations file and then map them\n",
    "item_label_cols = [col for col in relations.columns if 'item_label' in col]\n",
    "item_labels = relations[item_label_cols]\n",
    "item_labels = item_labels.stack()\n",
    "\n",
    "new_item_labels = _extend_id_map(item_labels,\n",
    "                                 item_label_id_map.index,\n",
    "                                 item_label_id_map.array,\n",
    "                                 dtype = type_id_dtype)\n",
    "\n",
    "if not new_item_labels.empty:\n",
    "    raise ValueError('Found the following item labels in the entry relations '\n",
    "                     'file which are expected from the entry prefixes but do '\n",
    "                     'not exist in the input data:\\n{}'\n",
    "                     .format(new_item_labels))\n",
    "\n",
    "for column in item_label_cols:\n",
    "    relations[column] = relations[column].map(item_label_id_map)\n",
    "\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 40]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_string_id</th>\n",
       "      <th>tgt_string_id</th>\n",
       "      <th>link_type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>48</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>52</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>52</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>52</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   src_string_id  tgt_string_id  link_type_id\n",
       "0              7              0             0\n",
       "1              7              1             0\n",
       "2              7              2             1\n",
       "3              7              3             2\n",
       "4              7              4             3\n",
       "5              7              5             4\n",
       "6              7              6             5\n",
       "8             12              0             0\n",
       "9             12              1             0\n",
       "10            12              8             1\n",
       "11            12              9             2\n",
       "12            12             10             3\n",
       "13            12             11             4\n",
       "14            12              6             5\n",
       "16            18             13             0\n",
       "17            18             14             1\n",
       "18            18             15             2\n",
       "19            18             16             3\n",
       "20            18             17             5\n",
       "22            22              1             0\n",
       "23            22             19             1\n",
       "24            22             20             3\n",
       "25            22             21             6\n",
       "27            25             23             0\n",
       "28            25             19             1\n",
       "29            25             24             6\n",
       "31            28             26             7\n",
       "32            28             27             6\n",
       "48            44             42             1\n",
       "49            44             43             5\n",
       "51            48             45             0\n",
       "52            48             46             1\n",
       "53            48             47             5\n",
       "55            52              0             0\n",
       "56            52              1             0\n",
       "57            52             49             1\n",
       "58            52             50             2\n",
       "59            52             16             3\n",
       "60            52             51             4\n",
       "61            52              6             5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _make_entry_assertions(grp, dropna=False):\n",
    "    lt_isna = grp['link_type_id'].isna()\n",
    "    lt_notna = ~lt_isna\n",
    "    if lt_notna.any():\n",
    "        src = grp.loc[lt_isna, 'string_id'].squeeze()\n",
    "        return pd.Series([src]*len(grp))\n",
    "    else:\n",
    "        return pd.Series([pd.NA]*len(grp))\n",
    "\n",
    "\n",
    "assertions = parsed_shnd.groupby(by=['csv_row', 'csv_col'], group_keys=False)\n",
    "assertions = assertions.apply(_make_entry_assertions)\n",
    "assertions.index = parsed_shnd.index\n",
    "has_link = ~parsed_shnd['link_type_id'].isna()\n",
    "print(list(map(len, [assertions.loc[has_link], parsed_shnd.loc[has_link]])))\n",
    "\n",
    "assertions = pd.DataFrame(\n",
    "    {'src_string_id': assertions.loc[has_link].array,\n",
    "     'tgt_string_id': parsed_shnd.loc[has_link, 'string_id'],\n",
    "     'link_type_id': parsed_shnd.loc[has_link, 'link_type_id']}\n",
    ")\n",
    "assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assertion_id</th>\n",
       "      <th>src_string_id</th>\n",
       "      <th>tgt_string_id</th>\n",
       "      <th>ref_string_id</th>\n",
       "      <th>link_type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>52</td>\n",
       "      <td>49</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>52</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>52</td>\n",
       "      <td>51</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    assertion_id src_string_id  tgt_string_id  ref_string_id  link_type_id\n",
       "0              0             7             12              7             8\n",
       "1              1            52              7             52             8\n",
       "2              2             7             18              7             8\n",
       "3              3             7             22              7             8\n",
       "4              4             7             25              7             8\n",
       "..           ...           ...            ...            ...           ...\n",
       "59            59            52             49           <NA>             1\n",
       "60            60            52             50           <NA>             2\n",
       "61            61            52             16           <NA>             3\n",
       "62            62            52             51           <NA>             4\n",
       "63            63            52              6           <NA>             5\n",
       "\n",
       "[64 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################\n",
    "# MAKE ASSERTIONS\n",
    "###############################################################\n",
    "\n",
    "def _make_assertions_from_entry_relations():\n",
    "\n",
    "\n",
    "    def _get_prefix_pairs(side):\n",
    "        '''\n",
    "        Reconstruct prefix pairs, including duplicates not present in\n",
    "        the parsed shorthand\n",
    "        '''\n",
    "\n",
    "        csv_col = csv_column_id_map[side + '_entry']\n",
    "        label = side + '_entry_prefix'\n",
    "        relevant_cols = ['csv_row', 'csv_col', 'entry_prefix', 'item_label']\n",
    "\n",
    "        entries = parsed_shnd[relevant_cols].query('item_label.isna()') \\\n",
    "                                            .query('csv_col == @csv_col')\n",
    "        entries = entries.drop(['item_label'], axis=1)\n",
    "\n",
    "        entries = entries.rename(\n",
    "            columns = {'entry_prefix': label,\n",
    "                       'csv_row': 'string_csv_row',\n",
    "                       'csv_col': 'string_csv_col'}\n",
    "        )\n",
    "        \n",
    "        on_side_dplcts = dplct_entries.query('entry_csv_col == @csv_col')\n",
    "        on_side_dplcts = entries.merge(on_side_dplcts, how='right')\n",
    "\n",
    "        all_pfxs = pd.concat([entries, on_side_dplcts])\n",
    "        \n",
    "        # \"string\" csv indexes locating distinct strings are different\n",
    "        # from \"entry\" csv indexes, which locate entries with\n",
    "        # potentially duplicate string values. Copy the string csv\n",
    "        # indexes to fill in missing data after merger with duplicates.\n",
    "        missing_entry = all_pfxs['entry_csv_row'].isna()\n",
    "\n",
    "        string_csv_cols = ['string_csv_row', 'string_csv_col']\n",
    "        string_csv_idx = all_pfxs.loc[missing_entry, string_csv_cols]\n",
    "\n",
    "        entry_csv_cols = ['entry_csv_row', 'entry_csv_col']\n",
    "        all_pfxs.loc[missing_entry, entry_csv_cols] = string_csv_idx.to_numpy()\n",
    "\n",
    "        return all_pfxs\n",
    "        \n",
    "\n",
    "    def _copy_cross_duplicates(L_prefixes, R_prefixes):\n",
    "        '''\n",
    "        THIS FUNCTION MUTATES BOTH ARGUMENTS\n",
    "\n",
    "        gets prefixes for duplicate entries in one column whose distinct\n",
    "        string values are present in the other column\n",
    "        '''\n",
    "\n",
    "        L_missing_pfix = L_prefixes['left_entry_prefix'].isna()\n",
    "        L_subset = ['string_csv_col', 'string_csv_row', 'left_entry_prefix']\n",
    "\n",
    "        R_missing_pfix = R_prefixes['right_entry_prefix'].isna()\n",
    "        R_subset = ['string_csv_col', 'string_csv_row', 'right_entry_prefix']\n",
    "\n",
    "        # copy values from left to right\n",
    "        to_copy = L_prefixes[L_subset].drop_duplicates()\n",
    "        to_fill = R_prefixes.loc[R_missing_pfix, R_subset]\n",
    "\n",
    "        cross_dplcts = to_copy.merge(to_fill,\n",
    "                                     on=['string_csv_row', 'string_csv_col'],\n",
    "                                     how='right')\n",
    "        cross_dplcts = cross_dplcts['left_entry_prefix'].array\n",
    "\n",
    "        R_prefixes.loc[R_missing_pfix, 'right_entry_prefix'] = cross_dplcts\n",
    "\n",
    "        # copy values from right to left\n",
    "        to_copy = R_prefixes[R_subset].drop_duplicates()\n",
    "        to_fill = L_prefixes.loc[L_missing_pfix, L_subset]\n",
    "\n",
    "        cross_dplcts = to_copy.merge(to_fill,\n",
    "                                     on=['string_csv_row', 'string_csv_col'],\n",
    "                                     how='right')\n",
    "        cross_dplcts = cross_dplcts['right_entry_prefix'].array\n",
    "\n",
    "        L_prefixes.loc[L_missing_pfix, 'left_entry_prefix'] = cross_dplcts\n",
    "\n",
    "    L_prefixes = _get_prefix_pairs('left')\n",
    "    R_prefixes = _get_prefix_pairs('right')\n",
    "\n",
    "    _copy_cross_duplicates(L_prefixes, R_prefixes)\n",
    "\n",
    "    L_prefixes = L_prefixes.drop('entry_csv_col', axis=1)\n",
    "    R_prefixes = R_prefixes.drop('entry_csv_col', axis=1)\n",
    "\n",
    "    L_prefixes = L_prefixes.rename(\n",
    "        columns = {'string_csv_row':'L_str_csv_row',\n",
    "                   'string_csv_col':'L_str_csv_col'}\n",
    "    )\n",
    "\n",
    "    R_prefixes = R_prefixes.rename(\n",
    "        columns = {'string_csv_row':'R_str_csv_row',\n",
    "                   'string_csv_col':'R_str_csv_col'}\n",
    "    )\n",
    "\n",
    "    prefix_pairs = L_prefixes.merge(R_prefixes)\n",
    "\n",
    "\n",
    "    def _merge_relation_component_w_parsed_data(component_prefix,\n",
    "                                                relations_selector,\n",
    "                                                links=False):\n",
    "\n",
    "        relevant_relations_cols = ['left_entry_prefix',\n",
    "                                   'right_entry_prefix',\n",
    "                                   component_prefix + '_csv_col',\n",
    "                                   component_prefix + '_item_label']\n",
    "        \n",
    "        if links:\n",
    "            relevant_relations_cols.append('link_type_id')\n",
    "        \n",
    "        selected_relations = relations.loc[relations_selector]\n",
    "\n",
    "        component = prefix_pairs.merge(\n",
    "            selected_relations[relevant_relations_cols],\n",
    "            on = ['left_entry_prefix', 'right_entry_prefix']\n",
    "        )\n",
    "\n",
    "        component_csv_col = component_prefix + '_csv_col'\n",
    "        component = component.loc[~component[component_csv_col].isna()]\n",
    "\n",
    "        csv_labels = ['csv_row', 'csv_col']\n",
    "        is_L = (component[component_csv_col] == 'L')\n",
    "        is_R = (component[component_csv_col] == 'R')\n",
    "\n",
    "        left_indexes = component[['L_str_csv_row', 'L_str_csv_col']].loc[is_L]\n",
    "        left_indexes.columns = csv_labels\n",
    "        right_indexes = component[['R_str_csv_row', 'R_str_csv_col']].loc[is_R]\n",
    "        right_indexes.columns = csv_labels\n",
    "\n",
    "        csv_indexes = pd.concat([left_indexes, right_indexes]).sort_index()\n",
    "        \n",
    "        component[['csv_row', 'csv_col']] = csv_indexes.to_numpy()\n",
    "        del(csv_indexes)\n",
    "        \n",
    "        component = component.drop(\n",
    "            ['left_entry_prefix',\n",
    "             'right_entry_prefix',\n",
    "             'L_str_csv_row',\n",
    "             'L_str_csv_col',\n",
    "             'R_str_csv_row',\n",
    "             'R_str_csv_col',\n",
    "             component_csv_col],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        component = component.rename(\n",
    "            columns = {component_prefix + '_item_label': 'item_label'}\n",
    "        )\n",
    "\n",
    "        shnd_cols = ['csv_row',\n",
    "                     'csv_col',\n",
    "                     'item_label',\n",
    "                     'item_list_position',\n",
    "                     'string_id']\n",
    "\n",
    "        component = component.merge(parsed_shnd[shnd_cols], \n",
    "                                    on=['csv_row', 'csv_col', 'item_label'],\n",
    "                                    how='left')\n",
    "\n",
    "        return component\n",
    "\n",
    "\n",
    "    ##########################################################\n",
    "    # Get assertion string IDs for relations whose sources and \n",
    "    # targets are matched one to one\n",
    "    ##########################################################\n",
    "    relation_is_nonlist = relations['list_mode'].isna()\n",
    "    relation_is_one_to_one = (relations['list_mode'] == '1:1')\n",
    "    relation_selector = relation_is_nonlist | relation_is_one_to_one\n",
    "\n",
    "    sources = _merge_relation_component_w_parsed_data(\n",
    "        'src',\n",
    "        relation_selector,\n",
    "        links=True\n",
    "    )\n",
    "    sources = sources[['entry_csv_row', 'string_id', 'link_type_id']]\n",
    "    sources = sources.rename(\n",
    "        columns = {'string_id': 'src_string_id'}\n",
    "    )\n",
    "\n",
    "    targets = _merge_relation_component_w_parsed_data(\n",
    "        'tgt',\n",
    "        relation_selector\n",
    "    )\n",
    "    targets = targets['string_id'].rename('tgt_string_id')\n",
    "\n",
    "    if len(sources) != len(targets):\n",
    "        raise ValueError('Length mismatch between sources and targets for '\n",
    "                         'one-to-one relations')\n",
    "\n",
    "    references = _merge_relation_component_w_parsed_data(\n",
    "        'ref',\n",
    "        relation_selector\n",
    "    )\n",
    "    references = references[['entry_csv_row', 'string_id']]\n",
    "    references = references.rename(\n",
    "        columns = {'string_id': 'ref_string_id'}\n",
    "    )\n",
    "\n",
    "    one_to_one_assertions = pd.concat([sources, targets], axis=1)\n",
    "    one_to_one_assertions = one_to_one_assertions.merge(\n",
    "        references,\n",
    "        on = 'entry_csv_row',\n",
    "        how = 'left'\n",
    "    )\n",
    "    \n",
    "    ##########################################################\n",
    "    # Get assertion string IDs for relations whose sources and \n",
    "    # targets are not matched one to one\n",
    "    ##########################################################\n",
    "    relation_is_one_to_many = (relations['list_mode'] == '1:m')\n",
    "    relation_is_many_to_one = (relations['list_mode'] == 'm:1')\n",
    "    relation_is_many_to_many = (relations['list_mode'] == 'm:m')\n",
    "\n",
    "    relation_selector = relation_is_one_to_many | relation_is_many_to_one\n",
    "    relation_selector = relation_selector | relation_is_many_to_many\n",
    "\n",
    "    sources = _merge_relation_component_w_parsed_data(\n",
    "        'src',\n",
    "        relation_selector,\n",
    "        links=True\n",
    "    )\n",
    "    sources = sources[['entry_csv_row', 'string_id', 'link_type_id']]\n",
    "    sources = sources.drop_duplicates()\n",
    "    sources = sources.rename(\n",
    "        columns = {'string_id': 'src_string_id'}\n",
    "    )\n",
    "\n",
    "    targets = _merge_relation_component_w_parsed_data(\n",
    "        'tgt',\n",
    "        relation_selector\n",
    "    )\n",
    "    targets = targets[['entry_csv_row', 'string_id']].drop_duplicates()\n",
    "    targets = targets.rename(\n",
    "        columns = {'string_id': 'tgt_string_id'}\n",
    "    )\n",
    "\n",
    "    references = _merge_relation_component_w_parsed_data(\n",
    "        'ref',\n",
    "        relation_selector\n",
    "    )\n",
    "    references = references[['entry_csv_row', 'string_id']].drop_duplicates()\n",
    "    references = references.rename(\n",
    "        columns = {'string_id': 'ref_string_id'}\n",
    "    )\n",
    "\n",
    "    many_to_many_assertions = sources.merge(targets, on='entry_csv_row') \\\n",
    "                                     .merge(references, on='entry_csv_row')\n",
    "    many_to_many_assertions = many_to_many_assertions.drop(\n",
    "        'entry_csv_row',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    assertions = pd.concat([one_to_one_assertions, many_to_many_assertions])\n",
    "\n",
    "    assertions = assertions[\n",
    "        ['src_string_id',\n",
    "         'tgt_string_id',\n",
    "         'ref_string_id',\n",
    "         'link_type_id']\n",
    "    ]\n",
    "    \n",
    "    return assertions\n",
    "\n",
    "a = _make_assertions_from_entry_relations()\n",
    "assertions = pd.concat([a, assertions])\n",
    "    \n",
    "assertions = assertions.reset_index(drop=True).reset_index()\n",
    "assertions = assertions.rename(columns = {'index': 'assertion_id'})\n",
    "assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    UInt8\n",
       "b    UInt8\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bibliograph.util import non_intersecting_sequence\n",
    "\n",
    "def normalize_types(to_norm, template, strict=True, continue_idx=True):\n",
    "    '''\n",
    "    Create an object from to_norm that can be concatenated with template\n",
    "    such that the concatenated object and its index will have the same\n",
    "    dtypes as the template.\n",
    "\n",
    "    If template is pandas.DataFrame and to_norm is dict-like or\n",
    "    list-like, convert each element of to_norm to a pandas.Series with\n",
    "    dtypes that conform to the dtypes of a template dataframe and and\n",
    "    concatenate them together as columns in a dataframe.\n",
    "\n",
    "    If template is pandas.Series, convert to_norm to a Series with the\n",
    "    appropriate array dtype or, if to_norm is dict-like,\n",
    "    convert its values to an array with the appropriate dtype and if\n",
    "    both to_norm and template have numeric indexes, also normalize the\n",
    "    index dtype. If to_norm is dict-like and either to_norm or template\n",
    "    does not have a numeric index, use to_norm.keys() as the output\n",
    "    index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    to_norm : dict-like or list-like\n",
    "        A set of objects to treat as columns of an output\n",
    "        pandas.DataFrame and whose types will be coerced.\n",
    "\n",
    "    template : pandas.DataFrame or pandas.Series\n",
    "        Output dtypes will conform to template.dtypes and the output\n",
    "        index dtype will be template.index.dtype\n",
    "\n",
    "    strict : bool, default True\n",
    "        If True and to_norm is dict-like, only objects whose keys are\n",
    "        column labels in the template will be included as columns in the\n",
    "        output dataframe.\n",
    "\n",
    "        If True, to_norm is list-like, and template has N columns,\n",
    "        include only the first N elements of to_norm in the output\n",
    "        dataframe.\n",
    "\n",
    "        If False and to_norm is dict-like, normalize dtypes for objects\n",
    "        whose keys are column labels in the template and include the\n",
    "        other elements of to_norm as columns with dtypes inferred by\n",
    "        the pandas.Series constructor.\n",
    "\n",
    "        If False, to_norm is list-like, and template has N columns,\n",
    "        normalize dtypes for the first N elements of to_norm and include\n",
    "        the other elements of to_norm as columns with dtypes inferred by\n",
    "        the pandas.Series constructor. Labels for the extra columns in\n",
    "        the output dataframe will be integers counting from N.\n",
    "\n",
    "    continue_index : bool, default True\n",
    "        If True and template has a numerical index, the index of the\n",
    "        returned object will be a sequence of integers which fills\n",
    "        gaps in and/or extends to_norm.index\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Has as many rows as the longest element of to_norm.\n",
    "\n",
    "        If to_norm is dict-like and strict is True, output includes\n",
    "        only objects in to_norm whose keys are also column labels in the\n",
    "        template.\n",
    "\n",
    "        If to_norm is list-like and strict is True, output has the same\n",
    "        width as the template.\n",
    "\n",
    "        If strict is False, output has one column for each element in\n",
    "        to_norm.\n",
    "    '''\n",
    "\n",
    "    # get the size of to_norm\n",
    "\n",
    "    try:\n",
    "        # to_norm is treated as columns, so if it has a columns\n",
    "        # attribute, we want the size of that instead of the length\n",
    "        num_elements = len(to_norm.columns)\n",
    "    except AttributeError:\n",
    "        num_elements = len(to_norm)\n",
    "\n",
    "    try:\n",
    "        tmplt_columns = template.columns\n",
    "        # If the template is a dataframe, get its width.\n",
    "        num_tmplt_columns = len(tmplt_columns)\n",
    "\n",
    "    except AttributeError:\n",
    "        # Template is not dataframe-like.\n",
    "        # Treat it as 1D object with attributes dtype and index\n",
    "        templt_idx_is_nmrc = pd.api.types.is_numeric_dtype(template.index)\n",
    "\n",
    "        try:\n",
    "            assert to_norm.items\n",
    "            # to_norm is dict-like, so process its keys as an index\n",
    "\n",
    "            norm_keys_are_nmrc = pd.api.types.is_numeric_dtype(\n",
    "                to_norm.keys()[0]\n",
    "            )\n",
    "            if strict and norm_keys_are_nmrc and templt_idx_is_nmrc:\n",
    "                index = non_intersecting_sequence(\n",
    "                    to_norm.keys(),\n",
    "                    template.index\n",
    "                )\n",
    "            else:\n",
    "                index = to_norm.keys()\n",
    "\n",
    "            values = to_norm.values()\n",
    "\n",
    "        except AttributeError:\n",
    "            if strict and templt_idx_is_nmrc:\n",
    "                index = non_intersecting_sequence(num_elements, template.index)\n",
    "            else:\n",
    "                index = range(num_elements)\n",
    "\n",
    "            values = to_norm\n",
    "\n",
    "        index = pd.Index(index, dtype=template.index.dtype)\n",
    "        return pd.Series(values, index=index, dtype=template.dtype)\n",
    "\n",
    "    try:\n",
    "        # check if to_norm is dict-like\n",
    "        assert to_norm.items\n",
    "        # optionally get extra columns\n",
    "        if (num_elements > num_tmplt_columns) and not strict:\n",
    "            extra_columns = {\n",
    "                k: v for k, v in to_norm.items() if k not in num_tmplt_columns\n",
    "            }\n",
    "\n",
    "    except AttributeError:\n",
    "        if num_elements < num_tmplt_columns:\n",
    "            raise ValueError(\n",
    "                'If to_norm is list-like, to_norm must have at least '\n",
    "                'as many elements as there are columns in template.'\n",
    "            )\n",
    "        # optionally get extra columns\n",
    "        if (num_elements > num_tmplt_columns) and not strict:\n",
    "            extra_columns = dict(zip(\n",
    "                range(num_tmplt_columns, num_elements),\n",
    "                to_norm[num_tmplt_columns:]\n",
    "            ))\n",
    "        # make the list-like dict-like\n",
    "        to_norm = dict(zip(tmplt_columns, to_norm[:num_tmplt_columns]))\n",
    "\n",
    "    to_norm = [\n",
    "        pd.Series(v, dtype=template[k].dtype, name=k)\n",
    "        for k, v in to_norm.items() if k in tmplt_columns\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        to_norm += [pd.Series(v, name=k) for k, v in extra_columns.items()]\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    new_df = pd.concat(to_norm, axis='columns')\n",
    "\n",
    "    if continue_idx and pd.api.types.is_numeric_dtype(template.index.dtype):\n",
    "        index = non_intersecting_sequence(new_df.index, template.index)\n",
    "\n",
    "    new_df.index = pd.Index(index, dtype=template.index.dtype)\n",
    "    return new_df\n",
    "\n",
    "a = pd.DataFrame({'a':[1,2,3],'b':[10,20,30]})\n",
    "a = a.astype({'a':pd.UInt8Dtype(), 'b':pd.UInt8Dtype()})\n",
    "a.index = a.index.astype(pd.UInt8Dtype())\n",
    "\n",
    "b = pd.Series({'a':10,'b':100})\n",
    "#print(b.dtype)\n",
    "b = normalize_types(b, a)\n",
    "#pd.concat([a, b]).dtypes\n",
    "\n",
    "c = pd.Series([1,2,3], dtype=pd.UInt16Dtype())\n",
    "d = [4]\n",
    "d = normalize_types(d, c, strict=False)\n",
    "pd.concat([c, d])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    alpha\n",
      "1     beta\n",
      "2    gamma\n",
      "Name: b, dtype: string\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a      int64\n",
       "b     object\n",
       "ID     UInt8\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame({'a':[1,2,3], 'b':['alpha', 'beta', 'gamma']})\n",
    "a.astype({'a':pd.UInt16Dtype(), 'b':pd.StringDtype()})\n",
    "print(a['b'].astype(pd.StringDtype()))\n",
    "c = pd.Series([10, 20, 30], dtype=pd.UInt8Dtype(), name='ID')\n",
    "\n",
    "pd.concat([a, c], axis='columns').dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69eb92836b941e979072a76c7fcfffe5419cca933cedd02cfafbdfca1a93358c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
