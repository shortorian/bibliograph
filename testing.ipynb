{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibliograph as bg\n",
    "\n",
    "bibtex_fname = 'bibliograph/test_data/bibtex_test_data_short.bib'\n",
    "entry_syntax_fname = \"bibliograph/resources/default_bibtex_syntax.csv\"\n",
    "\n",
    "tn = bg.slurp_bibtex(\n",
    "    bibtex_fname,\n",
    "    entry_syntax_fname,\n",
    "    syntax_case_sensitive=True,\n",
    "    allow_redundant_items=False,\n",
    "    aliases_dict=None,\n",
    "    aliases_case_sensitive=True,\n",
    "    space_char='|',\n",
    "    na_string_values='!',\n",
    "    na_node_type='missing'\n",
    ")\n",
    "tn.resolve_assertions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.9.13, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: c:\\Users\\short\\Dropbox\\jd\\60-69 projects\\61 public\\61.02 bibliograph\n",
      "plugins: anyio-3.4.0\n",
      "collected 28 items\n",
      "\n",
      "bibliograph\\tests.py ............................                        [100%]\n",
      "\n",
      "============================= 28 passed in 8.77s ==============================\n"
     ]
    }
   ],
   "source": [
    "!pytest bibliograph/tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import bibliograph as bg\n",
    "    import pandas as pd\n",
    "\n",
    "    s = bg.Shorthand(\n",
    "        entry_syntax=\"bibliograph/resources/default_entry_syntax.csv\",\n",
    "        link_syntax=\"bibliograph/resources/default_link_syntax.csv\",\n",
    "        syntax_case_sensitive=False\n",
    "    )\n",
    "\n",
    "    parsed = s.parse_text(\n",
    "        'bibliograph/test_data/manual_annotation.shnd',\n",
    "        item_separator='__',\n",
    "        default_entry_prefix='wrk',\n",
    "        space_char='|',\n",
    "        na_string_values=['!', 'x'],\n",
    "        na_node_type='missing',\n",
    "        skiprows=2,\n",
    "        comment_char='#'\n",
    "    )\n",
    "\n",
    "    synthesized = parsed.synthesize_shorthand_entries('wrk', fill_spaces=True)\n",
    "\n",
    "    check = pd.Series([\n",
    "        'asmith_bwu__1999__s_bams__101__803__xxx',\n",
    "        'asmith_bwu__1998__s_bams__100__42__yyy',\n",
    "        'bjones__1975__s_jats__90__1__!',\n",
    "        'bwu__1989__t_long|title__x__80__!',\n",
    "        'Some|Author__1989__t_Title|With|\\\\#__x__x__!',\n",
    "        'asmith_bwu__2008__s_bams__110__1__zzz'\n",
    "    ])\n",
    "\n",
    "    synthesized == check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibliograph as bg\n",
    "import pandas as pd\n",
    "\n",
    "aliases_dict = {\n",
    "    'actor': 'bibliograph/test_data/aliases_actor.csv',\n",
    "    'work': 'bibliograph/test_data/aliases_work.csv'\n",
    "}\n",
    "\n",
    "constraints_fname = \"bibliograph/resources/default_link_constraints.csv\"\n",
    "\n",
    "tn = bg.slurp_shorthand(\n",
    "    'bibliograph/test_data/shorthand_for_auto_aliasing.shnd',\n",
    "    \"bibliograph/resources/default_entry_syntax.csv\",\n",
    "    \"bibliograph/resources/default_link_syntax.csv\",\n",
    "    syntax_case_sensitive=False,\n",
    "    aliases_dict=aliases_dict,\n",
    "    aliases_case_sensitive=False,\n",
    "    automatic_aliasing=True,\n",
    "    link_constraints_fname=constraints_fname,\n",
    "    links_excluded_from_edges=['alias', 'title', 'supertitle'],\n",
    "    item_separator='__',\n",
    "    space_char='|',\n",
    "    na_string_values='!',\n",
    "    na_node_type='missing',\n",
    "    default_entry_prefix='wrk',\n",
    "    comment_char='#',\n",
    ")\n",
    "\n",
    "output_link_types = ['cited', 'acknowledged']\n",
    "edge_subset = None\n",
    "include_prefixes=False\n",
    "include_references=False\n",
    "string_type='abbr'\n",
    "\n",
    "link_type_ids = [\n",
    "    tn.id_lookup('link_types', t) for t in output_link_types\n",
    "    if t in tn.link_types['link_type'].array\n",
    "]\n",
    "\n",
    "edge_subset = tn.edges\n",
    "\n",
    "if edge_subset is None:\n",
    "    edge_subset = tn.edges.loc[edge_subset].query(\n",
    "        'link_type_id.isin(@link_type_ids)'\n",
    "    )\n",
    "else:\n",
    "    edge_subset = tn.edges.query('link_type_id.isin(@link_type_ids)')\n",
    "\n",
    "def get_assertions_by_string_id(tn, string_ids, assertion_component):\n",
    "    \n",
    "    if not bg.util.iterable_note_string(assertion_component):\n",
    "        assertion_component = [assertion_component]\n",
    "\n",
    "    if assertion_component == ['all']:\n",
    "        src = True\n",
    "        tgt = True\n",
    "        ref = True\n",
    "\n",
    "    selection = pd.Series(False, index=tn.assertions.index)\n",
    "\n",
    "    if ('src' in assertion_component) or src:\n",
    "        selection = selection | tn.assertions['src_string_id'].isin(string_ids)\n",
    "        \n",
    "    if ('tgt' in assertion_component) or tgt:\n",
    "        selection = selection | tn.assertions['tgt_string_id'].isin(string_ids)\n",
    "        \n",
    "    if ('ref' in assertion_component) or ref:\n",
    "        selection = selection | tn.assertions['ref_string_id'].isin(string_ids)\n",
    "\n",
    "def get_metadata_by_string_id(tn, string_ids, assertion_component):\n",
    "    \n",
    "    metadata_node_types = tn.node_types.query('has_metadata').index\n",
    "    nodes = tn.strings.loc[string_ids, 'node_id']\n",
    "    nodes = tn.nodes.loc[nodes].query(\n",
    "        'node_type_id.isin(@metadata_node_types)'\n",
    "    )\n",
    "\n",
    "    metadata_table_names = tn.get_node_types_by_node_id(nodes.index)\n",
    "    metadata_table_names = metadata_table_names.reset_index(name='name')\n",
    "    \n",
    "    node_id_groups = {\n",
    "        n: metadata_table_names.query('name == @n')['index'].array\n",
    "        for n in metadata_table_names['name'].unique()\n",
    "    }\n",
    "\n",
    "    metadata = {\n",
    "        k: tn.__getattr__(k).query('node_id.isin(@v)')\n",
    "        for k, v in node_id_groups.items()\n",
    "    }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "if include_prefixes:\n",
    "\n",
    "    tgt_inp_string_ids = get_assertions_by_string_id(\n",
    "        tn,\n",
    "        edge_subset['tgt_string_id'],\n",
    "        assertion_component='tgt'\n",
    "    )\n",
    "    tgt_inp_string_ids = tgt_inp_string_ids['inp_string_id']\n",
    "\n",
    "\n",
    "\n",
    "    if include_references:\n",
    "        pass\n",
    "\n",
    "edge_subset = tn.resolve_edges().loc[edge_subset.index]\n",
    "output = edge_subset.groupby('src_string').apply(\n",
    "    lambda x: '\\n'.join(['    , {}'.format(s) for s in x['tgt_string']])\n",
    ")\n",
    "output = output.reset_index().apply(lambda x: '{},\\n{}'.format(x['src_string'], x[0]), axis='columns')\n",
    "output = '\\n'.join(output)\n",
    "print(output)\n",
    "get_metadata_by_string_id(tn, tn.strings.index, 'all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "815b2be54127abaf32a64efea7cd3ff17cdf2bced9eda4b4c37568bb3e1c75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
