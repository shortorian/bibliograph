{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibliograph as bg\n",
    "import pandas as pd\n",
    "\n",
    "aliases_dict = {\n",
    "    'actor': 'bibliograph/test_data/aliases_actor.csv',\n",
    "    'work': 'bibliograph/test_data/aliases_work.csv'\n",
    "}\n",
    "\n",
    "tn = bg.slurp_shorthand(\n",
    "    'bibliograph/test_data/shorthand_with_aliases.shnd',\n",
    "    \"bibliograph/resources/default_entry_syntax.csv\",\n",
    "    \"bibliograph/resources/default_link_syntax.csv\",\n",
    "    syntax_case_sensitive=False,\n",
    "    aliases_dict=aliases_dict,\n",
    "    aliases_case_sensitive=False,\n",
    "    item_separator='__',\n",
    "    space_char='|',\n",
    "    na_string_values='!',\n",
    "    na_node_type='missing',\n",
    "    default_entry_prefix='wrk',\n",
    "    skiprows=2,\n",
    "    comment_char='#',\n",
    ")\n",
    "\n",
    "tn.resolve_assertions().query('link_type == \"alias\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibliograph as bg\n",
    "\n",
    "bibtex_fname = 'bibliograph/test_data/bibtex_test_data_short.bib'\n",
    "entry_syntax_fname = \"bibliograph/resources/default_bibtex_syntax.csv\"\n",
    "\n",
    "tn = bg.slurp_bibtex(\n",
    "    bibtex_fname,\n",
    "    entry_syntax_fname,\n",
    "    syntax_case_sensitive=True,\n",
    "    allow_redundant_items=False,\n",
    "    aliases_dict=None,\n",
    "    aliases_case_sensitive=True,\n",
    "    space_char='|',\n",
    "    na_string_values='!',\n",
    "    na_node_type='missing'\n",
    ")\n",
    "tn.resolve_assertions().query('link_type == \"alias\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate works when building nodes.\n",
    "\n",
    "A work is an entry node type, defined in the entry syntax. \n",
    "  - A work has an item of node type \"identifier\" with a link to the parent entry of type \"doi\"\n",
    "  - A work has items of node type \"work\" with links to the parent entry of types \"volume\", \"page\", and \"supertitle\"/\"title\"\n",
    "  - A work has an item of node type \"date\" with a link to the parent entry of type \"published\"\n",
    "\n",
    "IF two assertions exist between different strings of node type \"work\" and the same string of node type \"identifier\",\n",
    "  - THEN the work strings should map to the same node ID\n",
    "\n",
    "IF two assertions of link type \"doi\" exist between different strings of node type \"work\" and different strings of node type \"identifier\", \n",
    "  - AND the identifiers have strings in common that are the same after stripping any leading substrings which end in one of 'doi:', 'doi.org/', or 'doi/'\n",
    "  - THEN the work strings should map to the same node ID\n",
    "  - __to make this work, you build a set of aliases for the identifier nodes first and then check for strings in common__\n",
    "\n",
    "IF three assertions of link type (\"title\" or \"supertitle\"), \"volume\", and \"page\" exist between different strings of node type \"work\" and the same strings,\n",
    "  - THEN the work strings should map to the same node ID\n",
    "\n",
    "IF three assertions of link type \"published\", \"volume\", and \"page\" exist between different strings of node type \"work\" and the same strings,\n",
    "  - THEN the work strings should PROBABLY map to the same node ID\n",
    "\n",
    "IF there are two strings that map to nodes of the same type after applying a specified transformation, the strings should map to the same node ID\n",
    "  - __to make this work, you build a set of aliases using the transformation first and then build nodes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def apply_alias_generator(string_series, func):\n",
    "    \n",
    "    aliases = pd.DataFrame({\n",
    "        'string': string_series,\n",
    "        'alias': string_series.map(func)\n",
    "    })\n",
    "    \n",
    "    return aliases.dropna()\n",
    "\n",
    "def western_surname_alias_generator_serial(\n",
    "    name,\n",
    "    drop_nouns=['ms', 'mr', 'dr'],\n",
    "    generationals=['jr', 'sr'],\n",
    "    partial_surnames=['st', 'de', 'le', 'van', 'von']\n",
    "):\n",
    "\n",
    "    if ',' not in name:\n",
    "        return pd.NA\n",
    "\n",
    "    name = name.casefold()\n",
    "\n",
    "    drop_nouns = [s for s in drop_nouns if s in name]\n",
    "    drop_nouns = [s + '.' if s + '.' in name else s for s in drop_nouns]\n",
    "\n",
    "    generationals = [s for s in generationals if s in name]\n",
    "    generationals = [s + '.' if s + '.' in name else s for s in generationals]\n",
    "\n",
    "    partial_surnames = [s for s in partial_surnames if s in name]\n",
    "    partial_surnames = [\n",
    "        s + '.' if s + '.' in name else s for s in partial_surnames\n",
    "    ]\n",
    "\n",
    "    name = name.split(',')\n",
    "    name = [n.strip() for n in name]\n",
    "\n",
    "    if name[1] in drop_nouns:\n",
    "        \n",
    "        if len(name) == 2:\n",
    "            name = name[0].rsplit(' ', maxsplit=1)\n",
    "            name = [name[1], name[0]]\n",
    "        \n",
    "        else:\n",
    "            return pd.NA\n",
    "\n",
    "    if name[1] in generationals:\n",
    "        \n",
    "        if len(name) == 2:\n",
    "            g = name[1]\n",
    "            name = name[0].rsplit(' ', maxsplit=1)\n",
    "            name = [name[1], name[0]]\n",
    "            name[1] = name[1] + ' ' + g\n",
    "        \n",
    "        else:\n",
    "            return pd.NA\n",
    "\n",
    "    for m in drop_nouns:\n",
    "        name = [n.removeprefix(m) for n in name]\n",
    "        name = [n.removesuffix(m) for n in name]\n",
    "\n",
    "    name = [n.strip() for n in name]\n",
    "    \n",
    "    for p in partial_surnames:\n",
    "\n",
    "        if name[1].endswith(' ' + p):\n",
    "\n",
    "            name[0] = p + ' ' + name[0]\n",
    "            name[1] = name[1][:-len(p)]\n",
    "\n",
    "    name[0] = ''.join([c for c in name[0] if c.isalpha()])\n",
    "\n",
    "    name[1] = [\n",
    "        s.strip()[0]\n",
    "        for substring in name[1].split(' ')\n",
    "        for s in substring.split('-')\n",
    "        if s != ''\n",
    "    ]\n",
    "\n",
    "    return (name[0] + ''.join(name[1]))\n",
    "\n",
    "def western_surname_alias_generator_vector(\n",
    "    name_series,\n",
    "    drop_nouns=['ms', 'mrs', 'mr', 'dr', 'sir', 'dame'],\n",
    "    generationals=['jr', 'sr'],\n",
    "    partial_surnames=['st', 'de', 'le', 'van', 'von']\n",
    "):\n",
    "\n",
    "    names = name_series.copy().loc[name_series.str.contains(',')]\n",
    "\n",
    "    names = names.str.casefold()\n",
    "\n",
    "    names = names.str.split(',', expand=True)\n",
    "    names = names.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    if len(names.columns) > 2:\n",
    "        more_fields = names[2].notna()\n",
    "\n",
    "    else:\n",
    "        more_fields = pd.Series(False, index=names[0].index)\n",
    "    \n",
    "    names = names[[0, 1]]\n",
    "    \n",
    "    drop_nouns = pd.Series(drop_nouns)\n",
    "    drop_nouns = pd.concat([drop_nouns, drop_nouns.map(lambda x: x + '.')])\n",
    "    is_drop_noun = names[1].isin(drop_nouns)\n",
    "    \n",
    "    if is_drop_noun.any():\n",
    "\n",
    "        selection = names[0].loc[is_drop_noun & ~more_fields].copy()\n",
    "        selection = selection.str.rsplit(' ', n=1, expand=True)\n",
    "\n",
    "        names[0].loc[selection.index] = selection[1]\n",
    "        names[1].loc[selection.index] = selection[0]\n",
    "        \n",
    "        names[1].loc[is_drop_noun & more_fields] = pd.NA\n",
    " \n",
    "    generationals = pd.Series(generationals)\n",
    "    generationals = pd.concat([\n",
    "        generationals,\n",
    "        generationals.map(lambda x: x + '.')\n",
    "    ])\n",
    "    is_generational = names[1].isin(generationals)\n",
    "\n",
    "    if is_generational.any():\n",
    "        \n",
    "        gens = names[1].loc[is_generational & ~more_fields].copy()\n",
    "\n",
    "        selection = names[0].loc[is_generational & ~more_fields].copy()\n",
    "        selection = selection.str.rsplit(' ', n=1, expand=True)\n",
    "        slctn_idx = selection.index\n",
    "\n",
    "        names[0].loc[slctn_idx] = selection[1]\n",
    "        names[1].loc[slctn_idx] = selection[0]\n",
    "        names[1].loc[slctn_idx] = names[1].loc[slctn_idx] + ' ' + gens\n",
    "        \n",
    "        names[1].loc[is_generational & more_fields] = pd.NA\n",
    "\n",
    "    for m in drop_nouns:\n",
    "        names = names.apply(lambda x: x.str.removeprefix(m))\n",
    "        names = names.apply(lambda x: x.str.removesuffix(m))\n",
    "\n",
    "    names = names.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    partial_surnames = partial_surnames + [p + '.' for p in partial_surnames]\n",
    "\n",
    "    for p in partial_surnames:\n",
    "\n",
    "        endswith_p = names[1].str.endswith(' ' + p).fillna(False)\n",
    "\n",
    "        names[0].loc[endswith_p] = p + ' ' + names[0].loc[endswith_p]\n",
    "        names[1].loc[endswith_p] = names[1].loc[endswith_p].str.slice(\n",
    "            stop=-len(p)\n",
    "        )\n",
    "\n",
    "    names[0] = names[0].str.replace(r'[^\\w]|[\\d_]', '', regex=True)\n",
    "    names[1] = names[1].str.replace(r'(?!\\b)\\w*|\\W*?', '', regex=True)\n",
    "    \n",
    "    aliases = (names[0] + names[1]).str.casefold()\n",
    "    aliases = pd.concat([\n",
    "        aliases,\n",
    "        pd.Series(pd.NA, index=name_series.index.difference(aliases.index))\n",
    "    ])\n",
    "\n",
    "    return aliases.sort_index()\n",
    "\n",
    "names = pd.Series([\n",
    "    'Loon, H. van',\n",
    "    'van Loon, h.',\n",
    "    'van Loon, Harry',\n",
    "    'VAN LOON, H',\n",
    "    'Van loon, ',\n",
    "    'some other person',\n",
    "    'Rodr√≠guez-Silva, Ileana',\n",
    "    'nasa',\n",
    "    'Martin Luther King, jr.',\n",
    "    'King, Martin Luther jr.',\n",
    "    'Mr. Martin Luther King, jr.',\n",
    "    'St. Whatever, Given Name',\n",
    "    'Whatever, Given Name St.',\n",
    "    'University of Washington, Seattle',\n",
    "    'University of Chicago',\n",
    "    'Ms. Gerould, Joanne',\n",
    "    'Gerould, Ms. Joanne',\n",
    "    'Gerould, Joanne, Ms.',\n",
    "    'Joanne Gerould, Ms.',\n",
    "    'Surname, Compound Given-Name',\n",
    "    'Monde, Alice le',\n",
    "    'le Monde, Alice'\n",
    "])\n",
    "\n",
    "'''serial = apply_alias_generator(names, western_surname_alias_generator_serial)\n",
    "vector = western_surname_alias_generator_vector(names)\n",
    "vector = vector.dropna().rename('vectorized')\n",
    "pd.concat([serial, vector], axis='columns')'''\n",
    "serial = names.map(western_surname_alias_generator_serial)\n",
    "vector = western_surname_alias_generator_vector(names)\n",
    "((serial == vector) | (serial.isna() & vector.isna())).all()\n",
    "pd.concat([names, western_surname_alias_generator_vector(names)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def doi_alias_generator(doi_series, delimiters=['doi:', 'doi.org/', 'doi/']):\n",
    "\n",
    "    has_delimiter = pd.concat(\n",
    "        [doi_series.str.contains(d).rename(d) for d in delimiters],\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "    has_delimiter = has_delimiter.apply(\n",
    "        lambda x: pd.Series(x.name, index=has_delimiter.index).where(x)\n",
    "    )\n",
    "    has_delimiter = has_delimiter.ffill(axis='columns')\n",
    "    has_delimiter = has_delimiter[has_delimiter.columns[-1]]\n",
    "    \n",
    "    output = pd.concat(\n",
    "        [doi_series.rename('string'), has_delimiter.rename('delimiter')],\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "    def delimiter_splitter(delimiter_group):\n",
    "        \n",
    "        delimiter = delimiter_group.name\n",
    "\n",
    "        if pd.isna(delimiter):\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    'string': pd.NA,\n",
    "                    'delimiter': delimiter\n",
    "                },\n",
    "                index=delimiter_group.index\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            strings = delimiter_group['string'].str.split(\n",
    "               delimiter,\n",
    "               expand=True\n",
    "            )\n",
    "            return pd.DataFrame({\n",
    "                'string': strings[1].str.strip(),\n",
    "                'delimiter': delimiter\n",
    "            })\n",
    "\n",
    "    output = output.groupby(by='delimiter', dropna=False)\n",
    "    output = output.apply(delimiter_splitter)\n",
    "\n",
    "    return output['string'].rename(None)\n",
    "\n",
    "identifiers = pd.Series([\n",
    "    'xxx',\n",
    "    'yyy',\n",
    "    'zzz',\n",
    "    'doi:yyy',\n",
    "    'https://doi.org/zzz',\n",
    "    'doi/yyy'\n",
    "])\n",
    "'''aliases = doi_alias_generator(identifiers)\n",
    "output = pd.concat([identifiers, aliases], axis='columns')\n",
    "output = output.rename(columns={0: 'string', 1: 'alias'})\n",
    "output'''\n",
    "doi_alias_generator(identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibliograph as bg\n",
    "\n",
    "aliases_dict = {\n",
    "    'actor': 'bibliograph/test_data/aliases_actor.csv',\n",
    "    'work': 'bibliograph/test_data/aliases_work.csv'\n",
    "}\n",
    "\n",
    "tn = bg.slurp_shorthand(\n",
    "    'bibliograph/test_data/shorthand_with_aliases.shnd',\n",
    "    \"bibliograph/resources/default_entry_syntax.csv\",\n",
    "    \"bibliograph/resources/default_link_syntax.csv\",\n",
    "    syntax_case_sensitive=False,\n",
    "    aliases_dict=aliases_dict,\n",
    "    aliases_case_sensitive=False,\n",
    "    item_separator='__',\n",
    "    space_char='|',\n",
    "    na_string_values='!',\n",
    "    na_node_type='missing',\n",
    "    default_entry_prefix='wrk',\n",
    "    skiprows=2,\n",
    "    comment_char='#',\n",
    ")\n",
    "\n",
    "node_2_aliases = [\n",
    "    'NASA',\n",
    "    'National Aeronautics and Space Administration',\n",
    "    'nasa',\n",
    "    'national aeronautics and space administration'\n",
    "]\n",
    "\n",
    "strings_with_node_2 = tn.strings.loc[tn.strings['node_id'] == 2, 'string']\n",
    "\n",
    "assert (strings_with_node_2 == node_2_aliases).all().all()\n",
    "\n",
    "tn.resolve_strings().sort_values(by='node_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest bibliograph/tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibliograph as bg\n",
    "import pandas as pd\n",
    "\n",
    "aliases_dict = {\n",
    "    'actor': 'bibliograph/test_data/aliases_actor.csv',\n",
    "    'work': 'bibliograph/test_data/aliases_work.csv'\n",
    "}\n",
    "\n",
    "tn = bg.slurp_shorthand(\n",
    "    'bibliograph/test_data/shorthand_with_aliases.shnd',\n",
    "    \"bibliograph/resources/default_entry_syntax.csv\",\n",
    "    \"bibliograph/resources/default_link_syntax.csv\",\n",
    "    syntax_case_sensitive=False,\n",
    "    aliases_dict=aliases_dict,\n",
    "    aliases_case_sensitive=False,\n",
    "    automatic_aliasing=True,\n",
    "    item_separator='__',\n",
    "    space_char='|',\n",
    "    na_string_values='!',\n",
    "    na_node_type='missing',\n",
    "    default_entry_prefix='wrk',\n",
    "    skiprows=2,\n",
    "    comment_char='#',\n",
    ")\n",
    "\n",
    "tn.resolve_assertions().query('link_type == \"alias\"')\n",
    "tn.resolve_strings().sort_values(by='node_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "815b2be54127abaf32a64efea7cd3ff17cdf2bced9eda4b4c37568bb3e1c75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
