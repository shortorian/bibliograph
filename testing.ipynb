{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibliograph as bg\n",
    "import pandas as pd\n",
    "\n",
    "aliases_dict = {\n",
    "    'actor': 'bibliograph/test_data/aliases_actor.csv',\n",
    "    'work': 'bibliograph/test_data/aliases_work.csv'\n",
    "}\n",
    "\n",
    "tn = bg.slurp_shorthand(\n",
    "    'bibliograph/test_data/shorthand_with_aliases.shnd',\n",
    "    \"bibliograph/resources/default_entry_syntax.csv\",\n",
    "    \"bibliograph/resources/default_link_syntax.csv\",\n",
    "    syntax_case_sensitive=False,\n",
    "    aliases_dict=aliases_dict,\n",
    "    aliases_case_sensitive=False,\n",
    "    item_separator='__',\n",
    "    space_char='|',\n",
    "    na_string_values='!',\n",
    "    na_node_type='missing',\n",
    "    default_entry_prefix='wrk',\n",
    "    skiprows=2,\n",
    "    comment_char='#',\n",
    ")\n",
    "\n",
    "tn.resolve_assertions().query('link_type == \"alias\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibliograph as bg\n",
    "\n",
    "bibtex_fname = 'bibliograph/test_data/bibtex_test_data_short.bib'\n",
    "entry_syntax_fname = \"bibliograph/resources/default_bibtex_syntax.csv\"\n",
    "\n",
    "tn = bg.slurp_bibtex(\n",
    "    bibtex_fname,\n",
    "    entry_syntax_fname,\n",
    "    syntax_case_sensitive=True,\n",
    "    allow_redundant_items=False,\n",
    "    aliases_dict=None,\n",
    "    aliases_case_sensitive=True,\n",
    "    space_char='|',\n",
    "    na_string_values='!',\n",
    "    na_node_type='missing'\n",
    ")\n",
    "tn.resolve_assertions().query('link_type == \"alias\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate works when building nodes.\n",
    "\n",
    "A work is an entry node type, defined in the entry syntax. \n",
    "  - A work has an item of node type \"identifier\" with a link to the parent entry of type \"doi\"\n",
    "  - A work has items of node type \"work\" with links to the parent entry of types \"volume\", \"page\", and \"supertitle\"/\"title\"\n",
    "  - A work has an item of node type \"date\" with a link to the parent entry of type \"published\"\n",
    "\n",
    "IF two assertions exist between different strings of node type \"work\" and the same string of node type \"identifier\",\n",
    "  - THEN the work strings should map to the same node ID\n",
    "\n",
    "IF two assertions of link type \"doi\" exist between different strings of node type \"work\" and different strings of node type \"identifier\", \n",
    "  - AND the identifiers have strings in common that are the same after stripping any leading substrings which end in one of 'doi:', 'doi.org/', or 'doi/'\n",
    "  - THEN the work strings should map to the same node ID\n",
    "  - __to make this work, you build a set of aliases for the identifier nodes first and then check for strings in common__\n",
    "\n",
    "IF three assertions of link type (\"title\" or \"supertitle\"), \"volume\", and \"page\" exist between different strings of node type \"work\" and the same strings,\n",
    "  - THEN the work strings should map to the same node ID\n",
    "\n",
    "IF three assertions of link type \"published\", \"volume\", and \"page\" exist between different strings of node type \"work\" and the same strings,\n",
    "  - THEN the work strings should PROBABLY map to the same node ID\n",
    "\n",
    "IF there are two strings that map to nodes of the same type after applying a specified transformation, the strings should map to the same node ID\n",
    "  - __to make this work, you build a set of aliases using the transformation first and then build nodes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def apply_alias_generator(string_series, func):\n",
    "    \n",
    "    aliases = pd.DataFrame({\n",
    "        'string': string_series,\n",
    "        'alias': string_series.map(func)\n",
    "    })\n",
    "    \n",
    "    return aliases.dropna()\n",
    "\n",
    "def western_surname_alias_generator_serial(\n",
    "    name,\n",
    "    drop_nouns=['ms', 'mr', 'dr'],\n",
    "    generationals=['jr', 'sr'],\n",
    "    partial_surnames=['st', 'de', 'le', 'van', 'von']\n",
    "):\n",
    "\n",
    "    if ',' not in name:\n",
    "        return pd.NA\n",
    "\n",
    "    name = name.casefold()\n",
    "\n",
    "    drop_nouns = [s for s in drop_nouns if s in name]\n",
    "    drop_nouns = [s + '.' if s + '.' in name else s for s in drop_nouns]\n",
    "\n",
    "    generationals = [s for s in generationals if s in name]\n",
    "    generationals = [s + '.' if s + '.' in name else s for s in generationals]\n",
    "\n",
    "    partial_surnames = [s for s in partial_surnames if s in name]\n",
    "    partial_surnames = [\n",
    "        s + '.' if s + '.' in name else s for s in partial_surnames\n",
    "    ]\n",
    "\n",
    "    name = name.split(',')\n",
    "    name = [n.strip() for n in name]\n",
    "\n",
    "    if name[1] in drop_nouns:\n",
    "        \n",
    "        if len(name) == 2:\n",
    "            name = name[0].rsplit(' ', maxsplit=1)\n",
    "            name = [name[1], name[0]]\n",
    "        \n",
    "        else:\n",
    "            return pd.NA\n",
    "\n",
    "    if name[1] in generationals:\n",
    "        \n",
    "        if len(name) == 2:\n",
    "            g = name[1]\n",
    "            name = name[0].rsplit(' ', maxsplit=1)\n",
    "            name = [name[1], name[0]]\n",
    "            name[1] = name[1] + ' ' + g\n",
    "        \n",
    "        else:\n",
    "            return pd.NA\n",
    "\n",
    "    for m in drop_nouns:\n",
    "        name = [n.removeprefix(m) for n in name]\n",
    "        name = [n.removesuffix(m) for n in name]\n",
    "\n",
    "    name = [n.strip() for n in name]\n",
    "    \n",
    "    for p in partial_surnames:\n",
    "\n",
    "        if name[1].endswith(' ' + p):\n",
    "\n",
    "            name[0] = p + ' ' + name[0]\n",
    "            name[1] = name[1][:-len(p)]\n",
    "\n",
    "    name[0] = ''.join([c for c in name[0] if c.isalpha()])\n",
    "\n",
    "    name[1] = [\n",
    "        s.strip()[0]\n",
    "        for substring in name[1].split(' ')\n",
    "        for s in substring.split('-')\n",
    "        if s != ''\n",
    "    ]\n",
    "\n",
    "    return (name[0] + ''.join(name[1]))\n",
    "\n",
    "def western_surname_alias_generator_vector(\n",
    "    name_series,\n",
    "    drop_nouns=['ms', 'mrs', 'mr', 'dr', 'sir', 'dame'],\n",
    "    generationals=['jr', 'sr'],\n",
    "    partial_surnames=['st', 'de', 'le', 'van', 'von']\n",
    "):\n",
    "\n",
    "    names = name_series.copy().loc[name_series.str.contains(',')]\n",
    "\n",
    "    names = names.str.casefold()\n",
    "\n",
    "    names = names.str.split(',', expand=True)\n",
    "    names = names.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    if len(names.columns) > 2:\n",
    "        more_fields = names[2].notna()\n",
    "\n",
    "    else:\n",
    "        more_fields = pd.Series(False, index=names[0].index)\n",
    "    \n",
    "    names = names[[0, 1]]\n",
    "    \n",
    "    drop_nouns = pd.Series(drop_nouns)\n",
    "    drop_nouns = pd.concat([drop_nouns, drop_nouns.map(lambda x: x + '.')])\n",
    "    is_drop_noun = names[1].isin(drop_nouns)\n",
    "    \n",
    "    if is_drop_noun.any():\n",
    "\n",
    "        selection = names[0].loc[is_drop_noun & ~more_fields].copy()\n",
    "        selection = selection.str.rsplit(' ', n=1, expand=True)\n",
    "\n",
    "        names[0].loc[selection.index] = selection[1]\n",
    "        names[1].loc[selection.index] = selection[0]\n",
    "        \n",
    "        names[1].loc[is_drop_noun & more_fields] = pd.NA\n",
    " \n",
    "    generationals = pd.Series(generationals)\n",
    "    generationals = pd.concat([\n",
    "        generationals,\n",
    "        generationals.map(lambda x: x + '.')\n",
    "    ])\n",
    "    is_generational = names[1].isin(generationals)\n",
    "\n",
    "    if is_generational.any():\n",
    "        \n",
    "        gens = names[1].loc[is_generational & ~more_fields].copy()\n",
    "\n",
    "        selection = names[0].loc[is_generational & ~more_fields].copy()\n",
    "        selection = selection.str.rsplit(' ', n=1, expand=True)\n",
    "        slctn_idx = selection.index\n",
    "\n",
    "        names[0].loc[slctn_idx] = selection[1]\n",
    "        names[1].loc[slctn_idx] = selection[0]\n",
    "        names[1].loc[slctn_idx] = names[1].loc[slctn_idx] + ' ' + gens\n",
    "        \n",
    "        names[1].loc[is_generational & more_fields] = pd.NA\n",
    "\n",
    "    for m in drop_nouns:\n",
    "        names = names.apply(lambda x: x.str.removeprefix(m))\n",
    "        names = names.apply(lambda x: x.str.removesuffix(m))\n",
    "\n",
    "    names = names.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    partial_surnames = partial_surnames + [p + '.' for p in partial_surnames]\n",
    "\n",
    "    for p in partial_surnames:\n",
    "\n",
    "        endswith_p = names[1].str.endswith(' ' + p).fillna(False)\n",
    "\n",
    "        names[0].loc[endswith_p] = p + ' ' + names[0].loc[endswith_p]\n",
    "        names[1].loc[endswith_p] = names[1].loc[endswith_p].str.slice(\n",
    "            stop=-len(p)\n",
    "        )\n",
    "\n",
    "    names[0] = names[0].str.replace(r'[^\\w]|[\\d_]', '', regex=True)\n",
    "    names[1] = names[1].str.replace(r'(?!\\b)\\w*|\\W*?', '', regex=True)\n",
    "    \n",
    "    aliases = (names[0] + names[1]).str.casefold()\n",
    "    aliases = pd.concat([\n",
    "        aliases,\n",
    "        pd.Series(pd.NA, index=name_series.index.difference(aliases.index))\n",
    "    ])\n",
    "\n",
    "    return aliases.sort_index()\n",
    "\n",
    "names = pd.Series([\n",
    "    'Loon, H. van',\n",
    "    'van Loon, h.',\n",
    "    'van Loon, Harry',\n",
    "    'VAN LOON, H',\n",
    "    'Van loon, ',\n",
    "    'some other person',\n",
    "    'Rodr√≠guez-Silva, Ileana',\n",
    "    'nasa',\n",
    "    'Martin Luther King, jr.',\n",
    "    'King, Martin Luther jr.',\n",
    "    'Mr. Martin Luther King, jr.',\n",
    "    'St. Whatever, Given Name',\n",
    "    'Whatever, Given Name St.',\n",
    "    'University of Washington, Seattle',\n",
    "    'University of Chicago',\n",
    "    'Ms. Gerould, Joanne',\n",
    "    'Gerould, Ms. Joanne',\n",
    "    'Gerould, Joanne, Ms.',\n",
    "    'Joanne Gerould, Ms.',\n",
    "    'Surname, Compound Given-Name',\n",
    "    'Monde, Alice le',\n",
    "    'le Monde, Alice'\n",
    "])\n",
    "\n",
    "'''serial = apply_alias_generator(names, western_surname_alias_generator_serial)\n",
    "vector = western_surname_alias_generator_vector(names)\n",
    "vector = vector.dropna().rename('vectorized')\n",
    "pd.concat([serial, vector], axis='columns')'''\n",
    "serial = names.map(western_surname_alias_generator_serial)\n",
    "vector = western_surname_alias_generator_vector(names)\n",
    "((serial == vector) | (serial.isna() & vector.isna())).all()\n",
    "pd.concat([names, western_surname_alias_generator_vector(names)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def doi_alias_generator(doi_series, delimiters=['doi:', 'doi.org/', 'doi/']):\n",
    "\n",
    "    has_delimiter = pd.concat(\n",
    "        [doi_series.str.contains(d).rename(d) for d in delimiters],\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "    has_delimiter = has_delimiter.apply(\n",
    "        lambda x: pd.Series(x.name, index=has_delimiter.index).where(x)\n",
    "    )\n",
    "    has_delimiter = has_delimiter.ffill(axis='columns')\n",
    "    has_delimiter = has_delimiter[has_delimiter.columns[-1]]\n",
    "    \n",
    "    output = pd.concat(\n",
    "        [doi_series.rename('string'), has_delimiter.rename('delimiter')],\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "    def delimiter_splitter(delimiter_group):\n",
    "        \n",
    "        delimiter = delimiter_group.name\n",
    "\n",
    "        if pd.isna(delimiter):\n",
    "            return pd.DataFrame(\n",
    "                {\n",
    "                    'string': pd.NA,\n",
    "                    'delimiter': delimiter\n",
    "                },\n",
    "                index=delimiter_group.index\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            strings = delimiter_group['string'].str.split(\n",
    "               delimiter,\n",
    "               expand=True\n",
    "            )\n",
    "            return pd.DataFrame({\n",
    "                'string': strings[1].str.strip(),\n",
    "                'delimiter': delimiter\n",
    "            })\n",
    "\n",
    "    output = output.groupby(by='delimiter', dropna=False)\n",
    "    output = output.apply(delimiter_splitter)\n",
    "\n",
    "    return output['string'].rename(None)\n",
    "\n",
    "identifiers = pd.Series([\n",
    "    'xxx',\n",
    "    'yyy',\n",
    "    'zzz',\n",
    "    'doi:yyy',\n",
    "    'https://doi.org/zzz',\n",
    "    'doi/yyy'\n",
    "])\n",
    "'''aliases = doi_alias_generator(identifiers)\n",
    "output = pd.concat([identifiers, aliases], axis='columns')\n",
    "output = output.rename(columns={0: 'string', 1: 'alias'})\n",
    "output'''\n",
    "doi_alias_generator(identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>string</th>\n",
       "      <th>node_type</th>\n",
       "      <th>date_inserted</th>\n",
       "      <th>date_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>asmith</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>alice smith</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>smitha</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Alice Smith</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>beth wu</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>elizabeth wu</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>wue</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>wub</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Elizabeth Wu</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>Beth Wu</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bwu</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>NASA</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>national aeronautics and space administration</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>nasa</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>National Aeronautics and Space Administration</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>Bulletin of the American Meteorological Society</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>bulletin of the american meteorological society</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>bams</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1999</td>\n",
       "      <td>date</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>803</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>xxx</td>\n",
       "      <td>identifier</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>asmith_bwu__1999__bams__101__803__xxx</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>1998</td>\n",
       "      <td>date</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>42</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>yyy</td>\n",
       "      <td>identifier</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>Alice Smith_Elizabeth Wu__1998__Bulletin of th...</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14</td>\n",
       "      <td>1991</td>\n",
       "      <td>date</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>Project ABC TR no. 5</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16</td>\n",
       "      <td>!</td>\n",
       "      <td>missing</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17</td>\n",
       "      <td>NASA Technical Reports</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>18</td>\n",
       "      <td>NASA__1991__NASA Technical Reports__Project AB...</td>\n",
       "      <td>work</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19</td>\n",
       "      <td>University of Chicago</td>\n",
       "      <td>actor</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20</td>\n",
       "      <td>aff__National Aeronautics and Space Administra...</td>\n",
       "      <td>affiliation</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>tag</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>tag</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23</td>\n",
       "      <td>This is stuff shorthand ignores when you use s...</td>\n",
       "      <td>shorthand_text</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>24</td>\n",
       "      <td>Shorthand.parse_text</td>\n",
       "      <td>python_function</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>25</td>\n",
       "      <td>default_entry_prefix,item_separator,entry_pref...</td>\n",
       "      <td>shorthand_entry_syntax</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>26</td>\n",
       "      <td>left_entry_prefix,right_entry_prefix,source_po...</td>\n",
       "      <td>shorthand_link_syntax</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>27</td>\n",
       "      <td>bibliograph/test_data/shorthand_with_aliases.shnd</td>\n",
       "      <td>file_name</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>28</td>\n",
       "      <td>bibliograph/test_data/aliases_actor.csv</td>\n",
       "      <td>alias_reference</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>29</td>\n",
       "      <td>bibliograph/test_data/aliases_work.csv</td>\n",
       "      <td>alias_reference</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>30</td>\n",
       "      <td>bibliograph.core._insert_alias_assertions</td>\n",
       "      <td>python_function</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>31</td>\n",
       "      <td>&lt;_io.StringIO object at 0x000002BAF7AC0310&gt;</td>\n",
       "      <td>alias_reference</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32</td>\n",
       "      <td>&lt;_io.StringIO object at 0x000002BAED4C70D0&gt;</td>\n",
       "      <td>alias_reference</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>33</td>\n",
       "      <td>bibliograph.core._insert_alias_assertions</td>\n",
       "      <td>python_function</td>\n",
       "      <td>2022-08-05 17:29:47</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    node_id                                             string  \\\n",
       "0         0                                             asmith   \n",
       "35        0                                        alice smith   \n",
       "42        0                                             smitha   \n",
       "8         0                                        Alice Smith   \n",
       "34        1                                            beth wu   \n",
       "36        1                                       elizabeth wu   \n",
       "43        1                                                wue   \n",
       "44        1                                                wub   \n",
       "9         1                                       Elizabeth Wu   \n",
       "32        1                                            Beth Wu   \n",
       "1         1                                                bwu   \n",
       "16        2                                               NASA   \n",
       "37        2      national aeronautics and space administration   \n",
       "33        2                                               nasa   \n",
       "22        2      National Aeronautics and Space Administration   \n",
       "14        3    Bulletin of the American Meteorological Society   \n",
       "38        3    bulletin of the american meteorological society   \n",
       "6         3                                               bams   \n",
       "2         4                                               1999   \n",
       "3         5                                                101   \n",
       "4         6                                                803   \n",
       "5         7                                                xxx   \n",
       "7         8              asmith_bwu__1999__bams__101__803__xxx   \n",
       "10        9                                               1998   \n",
       "11       10                                                100   \n",
       "12       11                                                 42   \n",
       "13       12                                                yyy   \n",
       "15       13  Alice Smith_Elizabeth Wu__1998__Bulletin of th...   \n",
       "17       14                                               1991   \n",
       "18       15                               Project ABC TR no. 5   \n",
       "19       16                                                  !   \n",
       "20       17                             NASA Technical Reports   \n",
       "21       18  NASA__1991__NASA Technical Reports__Project AB...   \n",
       "23       19                              University of Chicago   \n",
       "24       20  aff__National Aeronautics and Space Administra...   \n",
       "25       21                                                  1   \n",
       "26       22                                                  2   \n",
       "27       23  This is stuff shorthand ignores when you use s...   \n",
       "28       24                               Shorthand.parse_text   \n",
       "29       25  default_entry_prefix,item_separator,entry_pref...   \n",
       "30       26  left_entry_prefix,right_entry_prefix,source_po...   \n",
       "31       27  bibliograph/test_data/shorthand_with_aliases.shnd   \n",
       "39       28            bibliograph/test_data/aliases_actor.csv   \n",
       "40       29             bibliograph/test_data/aliases_work.csv   \n",
       "41       30          bibliograph.core._insert_alias_assertions   \n",
       "45       31        <_io.StringIO object at 0x000002BAF7AC0310>   \n",
       "46       32        <_io.StringIO object at 0x000002BAED4C70D0>   \n",
       "47       33          bibliograph.core._insert_alias_assertions   \n",
       "\n",
       "                 node_type        date_inserted date_modified  \n",
       "0                    actor  2022-08-05 17:29:47          <NA>  \n",
       "35                   actor  2022-08-05 17:29:47          <NA>  \n",
       "42                   actor  2022-08-05 17:29:47          <NA>  \n",
       "8                    actor  2022-08-05 17:29:47          <NA>  \n",
       "34                   actor  2022-08-05 17:29:47          <NA>  \n",
       "36                   actor  2022-08-05 17:29:47          <NA>  \n",
       "43                   actor  2022-08-05 17:29:47          <NA>  \n",
       "44                   actor  2022-08-05 17:29:47          <NA>  \n",
       "9                    actor  2022-08-05 17:29:47          <NA>  \n",
       "32                   actor  2022-08-05 17:29:47          <NA>  \n",
       "1                    actor  2022-08-05 17:29:47          <NA>  \n",
       "16                   actor  2022-08-05 17:29:47          <NA>  \n",
       "37                   actor  2022-08-05 17:29:47          <NA>  \n",
       "33                   actor  2022-08-05 17:29:47          <NA>  \n",
       "22                   actor  2022-08-05 17:29:47          <NA>  \n",
       "14                    work  2022-08-05 17:29:47          <NA>  \n",
       "38                    work  2022-08-05 17:29:47          <NA>  \n",
       "6                     work  2022-08-05 17:29:47          <NA>  \n",
       "2                     date  2022-08-05 17:29:47          <NA>  \n",
       "3                     work  2022-08-05 17:29:47          <NA>  \n",
       "4                     work  2022-08-05 17:29:47          <NA>  \n",
       "5               identifier  2022-08-05 17:29:47          <NA>  \n",
       "7                     work  2022-08-05 17:29:47          <NA>  \n",
       "10                    date  2022-08-05 17:29:47          <NA>  \n",
       "11                    work  2022-08-05 17:29:47          <NA>  \n",
       "12                    work  2022-08-05 17:29:47          <NA>  \n",
       "13              identifier  2022-08-05 17:29:47          <NA>  \n",
       "15                    work  2022-08-05 17:29:47          <NA>  \n",
       "17                    date  2022-08-05 17:29:47          <NA>  \n",
       "18                    work  2022-08-05 17:29:47          <NA>  \n",
       "19                 missing  2022-08-05 17:29:47          <NA>  \n",
       "20                    work  2022-08-05 17:29:47          <NA>  \n",
       "21                    work  2022-08-05 17:29:47          <NA>  \n",
       "23                   actor  2022-08-05 17:29:47          <NA>  \n",
       "24             affiliation  2022-08-05 17:29:47          <NA>  \n",
       "25                     tag  2022-08-05 17:29:47          <NA>  \n",
       "26                     tag  2022-08-05 17:29:47          <NA>  \n",
       "27          shorthand_text  2022-08-05 17:29:47          <NA>  \n",
       "28         python_function  2022-08-05 17:29:47          <NA>  \n",
       "29  shorthand_entry_syntax  2022-08-05 17:29:47          <NA>  \n",
       "30   shorthand_link_syntax  2022-08-05 17:29:47          <NA>  \n",
       "31               file_name  2022-08-05 17:29:47          <NA>  \n",
       "39         alias_reference  2022-08-05 17:29:47          <NA>  \n",
       "40         alias_reference  2022-08-05 17:29:47          <NA>  \n",
       "41         python_function  2022-08-05 17:29:47          <NA>  \n",
       "45         alias_reference  2022-08-05 17:29:47          <NA>  \n",
       "46         alias_reference  2022-08-05 17:29:47          <NA>  \n",
       "47         python_function  2022-08-05 17:29:47          <NA>  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bibliograph as bg\n",
    "\n",
    "aliases_dict = {\n",
    "    'actor': 'bibliograph/test_data/aliases_actor.csv',\n",
    "    'work': 'bibliograph/test_data/aliases_work.csv'\n",
    "}\n",
    "\n",
    "tn = bg.slurp_shorthand(\n",
    "    'bibliograph/test_data/shorthand_with_aliases.shnd',\n",
    "    \"bibliograph/resources/default_entry_syntax.csv\",\n",
    "    \"bibliograph/resources/default_link_syntax.csv\",\n",
    "    syntax_case_sensitive=False,\n",
    "    aliases_dict=aliases_dict,\n",
    "    aliases_case_sensitive=False,\n",
    "    item_separator='__',\n",
    "    space_char='|',\n",
    "    na_string_values='!',\n",
    "    na_node_type='missing',\n",
    "    default_entry_prefix='wrk',\n",
    "    skiprows=2,\n",
    "    comment_char='#',\n",
    ")\n",
    "\n",
    "node_2_aliases = [\n",
    "    'NASA',\n",
    "    'National Aeronautics and Space Administration',\n",
    "    'nasa',\n",
    "    'national aeronautics and space administration'\n",
    "]\n",
    "\n",
    "strings_with_node_2 = tn.strings.loc[tn.strings['node_id'] == 2, 'string']\n",
    "\n",
    "assert (strings_with_node_2 == node_2_aliases).all().all()\n",
    "\n",
    "tn.resolve_strings().sort_values(by='node_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest bibliograph/tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibliograph as bg\n",
    "import pandas as pd\n",
    "\n",
    "aliases_dict = {\n",
    "    'actor': 'bibliograph/test_data/aliases_actor.csv',\n",
    "    'work': 'bibliograph/test_data/aliases_work.csv'\n",
    "}\n",
    "\n",
    "tn = bg.slurp_shorthand(\n",
    "    'bibliograph/test_data/shorthand_with_aliases.shnd',\n",
    "    \"bibliograph/resources/default_entry_syntax.csv\",\n",
    "    \"bibliograph/resources/default_link_syntax.csv\",\n",
    "    syntax_case_sensitive=False,\n",
    "    aliases_dict=aliases_dict,\n",
    "    aliases_case_sensitive=False,\n",
    "    item_separator='__',\n",
    "    space_char='|',\n",
    "    na_string_values='!',\n",
    "    na_node_type='missing',\n",
    "    default_entry_prefix='wrk',\n",
    "    skiprows=2,\n",
    "    comment_char='#',\n",
    ")\n",
    "\n",
    "tn.resolve_assertions().query('link_type == \"alias\"')\n",
    "tn.resolve_strings().sort_values(by='node_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "815b2be54127abaf32a64efea7cd3ff17cdf2bced9eda4b4c37568bb3e1c75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
